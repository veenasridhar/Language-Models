{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqEKvyga5oC4"
      },
      "source": [
        "<h1> Transformers </h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbZB8WM45kmu",
        "outputId": "1d5b8209-55e1-4f81-91dd-75b0ea853b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-27 08:55:08--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-09-27 08:55:09 (52.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukiFUIppA7zM",
        "outputId": "52348a52-81ce-4fd1-e920-3d5a94f4126f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The length of the dataset is  1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Opening the file\n",
        "with open ('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "print(\"The length of the dataset is \", len(text))\n",
        "print(text[:1000])  #First Thousand characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUzGmXX1BSne",
        "outputId": "36cbd316-794a-459e-96da-c0cc9888fa52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "#Generating the character vocabulary - list of unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))  #Printing the vocabulary List\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB0kyrv_CQkh",
        "outputId": "76eae927-815a-432a-807d-ab30518588b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding of Hello World! [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
            "Decode back to string Hello World!\n"
          ]
        }
      ],
      "source": [
        "#Creating a mapping from the strings to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]   #Encoder builds a list of integer from the string of characters\n",
        "decode = lambda n: \"\".join(itos[i] for i in n) #Decoder builds a string from a list of integers\n",
        "\n",
        "print(\"Encoding of Hello World!\", encode(\"Hello World!\"))\n",
        "print(\"Decode back to string\", decode(encode(\"Hello World!\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpzkJrbIC_6Y",
        "outputId": "393203eb-aa49-426c-c18c-1f5b53496687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "#Encoding the entire dataset to a Tensor\n",
        "import torch\n",
        "data_tensor = torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data_tensor.shape, data_tensor.dtype )\n",
        "print(data_tensor[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3IGH8zxEqM3"
      },
      "outputs": [],
      "source": [
        "#Splitting the data into train and validation sets\n",
        "n = int((0.8)*len(data_tensor))\n",
        "train_data = data_tensor[:n]\n",
        "val_data = data_tensor[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6MagkivIR3z",
        "outputId": "cb8e131b-1c0f-49b5-d118-a143b0fbc7c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When the input is tensor([18]) the predicted next charcter is 47\n",
            "When the input is tensor([18, 47]) the predicted next charcter is 56\n",
            "When the input is tensor([18, 47, 56]) the predicted next charcter is 57\n",
            "When the input is tensor([18, 47, 56, 57]) the predicted next charcter is 58\n",
            "When the input is tensor([18, 47, 56, 57, 58]) the predicted next charcter is 1\n",
            "When the input is tensor([18, 47, 56, 57, 58,  1]) the predicted next charcter is 15\n",
            "When the input is tensor([18, 47, 56, 57, 58,  1, 15]) the predicted next charcter is 47\n",
            "When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the predicted next charcter is 58\n"
          ]
        }
      ],
      "source": [
        "#Creating a block size which defines the maximum character range that model considers to predict the next characetr.\n",
        "#The model can take 1 to block size number of characters for its next prediction\n",
        "\n",
        "block_size = 8\n",
        "x = train_data[:block_size] # Input to the model\n",
        "y = train_data [1:block_size+1] # Predictions for the next character\n",
        "\n",
        "#Lets see an example\n",
        "for c in range(block_size):\n",
        "  context = x[:c+1]\n",
        "  target_prediction = y[c]\n",
        "  print(f\"When the input is {context} the predicted next charcter is {target_prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7pe6gTBLGPO"
      },
      "outputs": [],
      "source": [
        "#Batching the data\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4  #Number of independent sequences that will be processed in parallel\n",
        "block_size = 8 # Context length for predictions.\n",
        "\n",
        "def get_batch(split):\n",
        "  #Generates multiple small batch of data (input x and output y)\n",
        "  data = train_data if split == 'train' else val_data\n",
        "\n",
        "  #Create a tensor with random numbers within the data length. The random numbers in tensors will be the start index for each batch.\n",
        "  index = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  #randint returns a tensor with random integers generated between low(inclusive) and high (excluded) limits.\n",
        "  #Here low is the default 0 and the high is length(data) - block_size.\n",
        "  #The shape of the tensor is defined by size which is (batch_size,) i.e (4,)\n",
        "  #O/P: index = tensor([217291,  74725,  60587,  27397])\n",
        "\n",
        "  #Extracting the chracter blocks for the generated blocks (4 blocks in total).\n",
        "  #torch.stack concatenates the 4 blocks of tensors in a new dimension\n",
        "  #while torch.cat stacks each block into a single tensor with out a new dimension.\n",
        "  x = torch.stack([data[i: i+block_size] for i in index])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
        "  return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuySMeDAL_pQ",
        "outputId": "ac2d2347-f4aa-4871-ed4e-72ed86d4a354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  0, 39, 52, 42,  1, 45],\n",
            "        [ 1, 58, 46, 43,  1, 51, 43, 56],\n",
            "        [39, 56, 42, 43, 52,  7, 46, 53],\n",
            "        [47, 52, 45,  1, 42, 47, 57, 41]])\n",
            "-------\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[58,  0, 39, 52, 42,  1, 45, 47],\n",
            "        [58, 46, 43,  1, 51, 43, 56, 43],\n",
            "        [56, 42, 43, 52,  7, 46, 53, 59],\n",
            "        [52, 45,  1, 42, 47, 57, 41, 53]])\n"
          ]
        }
      ],
      "source": [
        "#Printing the inputs and its corresponding targets.\n",
        "\n",
        "xb,yb = get_batch(\"trian\")\n",
        "print(\"inputs:\")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "\n",
        "print(\"-------\")\n",
        "\n",
        "print(\"targets:\")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh3k8TUzT63J",
        "outputId": "710c0183-153a-4650-cc97-4ef77c62005e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If the character is  [43]\n",
            "The predicted target must be  tensor(58)\n",
            "If the character is  [43, 58]\n",
            "The predicted target must be  tensor(0)\n",
            "If the character is  [43, 58, 0]\n",
            "The predicted target must be  tensor(39)\n",
            "If the character is  [43, 58, 0, 39]\n",
            "The predicted target must be  tensor(52)\n",
            "If the character is  [43, 58, 0, 39, 52]\n",
            "The predicted target must be  tensor(42)\n",
            "If the character is  [43, 58, 0, 39, 52, 42]\n",
            "The predicted target must be  tensor(1)\n",
            "If the character is  [43, 58, 0, 39, 52, 42, 1]\n",
            "The predicted target must be  tensor(45)\n",
            "If the character is  [43, 58, 0, 39, 52, 42, 1, 45]\n",
            "The predicted target must be  tensor(47)\n",
            "------Next BLock--------\n",
            "If the character is  [1]\n",
            "The predicted target must be  tensor(58)\n",
            "If the character is  [1, 58]\n",
            "The predicted target must be  tensor(46)\n",
            "If the character is  [1, 58, 46]\n",
            "The predicted target must be  tensor(43)\n",
            "If the character is  [1, 58, 46, 43]\n",
            "The predicted target must be  tensor(1)\n",
            "If the character is  [1, 58, 46, 43, 1]\n",
            "The predicted target must be  tensor(51)\n",
            "If the character is  [1, 58, 46, 43, 1, 51]\n",
            "The predicted target must be  tensor(43)\n",
            "If the character is  [1, 58, 46, 43, 1, 51, 43]\n",
            "The predicted target must be  tensor(56)\n",
            "If the character is  [1, 58, 46, 43, 1, 51, 43, 56]\n",
            "The predicted target must be  tensor(43)\n",
            "------Next BLock--------\n",
            "If the character is  [39]\n",
            "The predicted target must be  tensor(56)\n",
            "If the character is  [39, 56]\n",
            "The predicted target must be  tensor(42)\n",
            "If the character is  [39, 56, 42]\n",
            "The predicted target must be  tensor(43)\n",
            "If the character is  [39, 56, 42, 43]\n",
            "The predicted target must be  tensor(52)\n",
            "If the character is  [39, 56, 42, 43, 52]\n",
            "The predicted target must be  tensor(7)\n",
            "If the character is  [39, 56, 42, 43, 52, 7]\n",
            "The predicted target must be  tensor(46)\n",
            "If the character is  [39, 56, 42, 43, 52, 7, 46]\n",
            "The predicted target must be  tensor(53)\n",
            "If the character is  [39, 56, 42, 43, 52, 7, 46, 53]\n",
            "The predicted target must be  tensor(59)\n",
            "------Next BLock--------\n",
            "If the character is  [47]\n",
            "The predicted target must be  tensor(52)\n",
            "If the character is  [47, 52]\n",
            "The predicted target must be  tensor(45)\n",
            "If the character is  [47, 52, 45]\n",
            "The predicted target must be  tensor(1)\n",
            "If the character is  [47, 52, 45, 1]\n",
            "The predicted target must be  tensor(42)\n",
            "If the character is  [47, 52, 45, 1, 42]\n",
            "The predicted target must be  tensor(47)\n",
            "If the character is  [47, 52, 45, 1, 42, 47]\n",
            "The predicted target must be  tensor(57)\n",
            "If the character is  [47, 52, 45, 1, 42, 47, 57]\n",
            "The predicted target must be  tensor(41)\n",
            "If the character is  [47, 52, 45, 1, 42, 47, 57, 41]\n",
            "The predicted target must be  tensor(53)\n",
            "------Next BLock--------\n"
          ]
        }
      ],
      "source": [
        "for i in range(batch_size):\n",
        "  for j in range(block_size):\n",
        "    print(\"If the character is \",xb[i,:j+1].tolist())\n",
        "    print(\"The predicted target must be \",yb[i,j])\n",
        "  print(\"------Next BLock--------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfPHVqS0XNBQ"
      },
      "source": [
        "<h2> Bigram Language Model </h2>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxygs5OFWrRU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "n_embed = 32\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    #self.token_embedding_table = nn.Embedding(vocab_size,vocab-size) # A simple lookup table that store word embeddings of fixed dictionary and size (A 65x65 lookup table)\n",
        "    #Here the length of lookup table is 65 and each embedding 65 length long where 65 is the vocab_size\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed) # A lookup table for each character with its 32length embedding\n",
        "\n",
        "    #We can also embed the positions of the characters - Positional Encoding\n",
        "    self.position_embedding_table = nn.Embedding(block_size,n_embed)#Each position index is transformed into a 32 character embedding\n",
        "\n",
        "    #A linear layer to transform the embeddings into logits\n",
        "    self.languagemodel_head = nn.Linear(n_embed,vocab_size) #n_embed is the input layer size and the vocab_size is the output layer size\n",
        "\n",
        "  def forward(self,inputs,targets = None):\n",
        "\n",
        "    token_emb = self.token_embedding_table(inputs) # Each character is transformed into a 32 character length embedding\n",
        "    #print(tok_emb) This is the first batch of eight characters where each character is a n_embed size embedding = (B,T,32) tensor eg.(32,8,32)\n",
        "\n",
        "    position_emb = self.position_embedding_table(torch.arange(8))#Each position index is transformed into a 32 character embedding\n",
        "    #print(position_emb) #The embedding of each position from 0 to 7 = (8,32) tensor\n",
        "\n",
        "    #print(token_emb.shape, position_emb.shape)\n",
        "    x = token_emb + position_emb  # Now the input has both the chracter and the positional imformation\n",
        "    logits = self.languagemodel_head(x)\n",
        "\n",
        "    #logits = self.token_embedding_table(inputs) # Logits is our prediction for the next character.\n",
        "    #Each character is converted into a 65 length embedding. Logits is of shape (Batch,Block(Time),Channels) = (4,8,64)\n",
        "    #inputs is a tensor of block of character indicies and not the character itself (4 batches of character blocks (8 characters))\n",
        "\n",
        "    #Calculating Loss on our predictions (logits) using the Cross Entropy loss\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      #The logits is a 3D tensor of shape (B,T,C) but cross_entropy loss function in pytorch expects the input to be a 2D tensor of shape (N,C) where N is number of batches and C is number of classes.\n",
        "      # Here the classes are  each embedding for a character. So we reshape the logits to (B*T, C) = (32,65) and out target output to be (B*T) = (32)\n",
        "      logits = logits.view(B*T, C) #view reshapes the tensor to the given shape\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits,loss\n",
        "\n",
        "\n",
        "#A simple bigram model to generate the next character (as many characters as max_new_tokens) based on its previous character.\n",
        "  def generate(self, indices, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      #get the prediction for next character based on the inputs\n",
        "      logits, loss = self(indices)\n",
        "      # Logits is of shape (B,T,C) where we are interested in the time dimension ie. the previous element only\n",
        "      logits = logits [:, -1, :] #Now the logits are of shape (B,C)\n",
        "      #Apply softmax to get the probabilities from the logits (Log odds)\n",
        "      prob = F.softmax(logits, dim = -1) # dim attribute tells the dimension across which the probability distribution is calculated\n",
        "\n",
        "      #Find the index of the next character based on the above probability distribution\n",
        "      next_char_idx = torch.multinomial(prob,max_new_tokens,replacement=True)\n",
        "      #append this predicted character index to the previous predicted chracters until we reach max new tokens\n",
        "      indicies = torch.cat((indices,next_char_idx),dim =1)\n",
        "    return indicies\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfut7RU2Out2",
        "outputId": "131501bf-6a8e-4177-979a-a07c8205ca10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.4019, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "pQRe'HEg?roHWrG!DG itDNERQESiRJmL3;LkfRmm':YRgvvvPH'Ed!lRLQa,j.Vk t,jagtUtS tvwDElEyHyZi'Nd3gVxxvEy!DVw!,A?Ry?VRRjVoRjNcCDlmaCptvLeC.vjMaMhyr\n",
            "bOgzygXz\n"
          ]
        }
      ],
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "output, loss  = m(xb,yb) # xb is the inputs and the yb is the targets\n",
        "print(output.shape)\n",
        "print(loss)\n",
        "inputs = torch.zeros((1,1),dtype = torch.long) # This is a 1x1 tensor with the index 0. As per the encoding vocab list it is a new line character.\n",
        "max_new_tokens = 150\n",
        "generated_tensor = m.generate(inputs,max_new_tokens) # This generates a tensor of (1,151) 1 batch of 151 character indicies.\n",
        "#So we convert the tensor to a list and decode the character indicies to characters.\n",
        "generated_sequence = decode(generated_tensor[0].tolist())\n",
        "print(generated_sequence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6Rh8mjXPgcg"
      },
      "outputs": [],
      "source": [
        "#Creating a optimizer for the model\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) #The m.parameters() refer to the weights and biases (iteratable model parameters) that get updated while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yezXt68ESXxX"
      },
      "outputs": [],
      "source": [
        "#Averaging the loss over multiple iterations and taking the average loss\n",
        "\n",
        "@torch.no_grad()\n",
        "#Disables gradient calculation even though the model inputs have requires_grad = True. This just to calculate the average loss and\n",
        "#.backward() will not be called on these calcuations. Saves the memory consumption by avaoiding to store all the intermediate results.\n",
        "\n",
        "def average_loss(iters):\n",
        "  out = {}\n",
        "  m.eval() # Switches the model to evaluation model where layers like dropout, BatchNorm do not have any effect\n",
        "  for split in ['train', 'val']: #Calculate the loss of data in both train and validation batch\n",
        "    losses = torch.zeros(iters) # Intializing the losses to zeros\n",
        "    for k in range (iters):\n",
        "      X,Y = get_batch(split)\n",
        "      logits,loss = m(X,Y)    #Calculating the logits and loss for the batch of input and outputs\n",
        "      losses[k] = loss.item() #Extracting the loss for kth iteration\n",
        "    out[split] = losses.mean()\n",
        "  m.train() # Switching the model back to training mode\n",
        "  return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w7-BLzoaQRVS",
        "outputId": "43a0c4df-6f47-400c-bea0-10e47d673d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': tensor(2.5892), 'val': tensor(2.6197)}\n",
            "{'train': tensor(2.5847), 'val': tensor(2.6281)}\n",
            "{'train': tensor(2.5854), 'val': tensor(2.6287)}\n",
            "{'train': tensor(2.5926), 'val': tensor(2.6286)}\n",
            "{'train': tensor(2.5897), 'val': tensor(2.6199)}\n",
            "{'train': tensor(2.5916), 'val': tensor(2.6288)}\n",
            "{'train': tensor(2.5993), 'val': tensor(2.6407)}\n",
            "{'train': tensor(2.5840), 'val': tensor(2.6184)}\n",
            "{'train': tensor(2.5653), 'val': tensor(2.6161)}\n",
            "{'train': tensor(2.5793), 'val': tensor(2.6027)}\n",
            "{'train': tensor(2.5818), 'val': tensor(2.6098)}\n",
            "{'train': tensor(2.5808), 'val': tensor(2.6158)}\n",
            "{'train': tensor(2.5736), 'val': tensor(2.6201)}\n",
            "{'train': tensor(2.5737), 'val': tensor(2.6089)}\n",
            "{'train': tensor(2.5836), 'val': tensor(2.6241)}\n",
            "{'train': tensor(2.5681), 'val': tensor(2.6261)}\n",
            "{'train': tensor(2.5960), 'val': tensor(2.6207)}\n",
            "{'train': tensor(2.5828), 'val': tensor(2.6119)}\n",
            "{'train': tensor(2.5829), 'val': tensor(2.6065)}\n",
            "{'train': tensor(2.5889), 'val': tensor(2.6176)}\n",
            "{'train': tensor(2.5883), 'val': tensor(2.6154)}\n",
            "{'train': tensor(2.5995), 'val': tensor(2.6346)}\n",
            "{'train': tensor(2.5935), 'val': tensor(2.6146)}\n",
            "{'train': tensor(2.5993), 'val': tensor(2.6341)}\n",
            "{'train': tensor(2.5936), 'val': tensor(2.6240)}\n",
            "{'train': tensor(2.5760), 'val': tensor(2.6219)}\n",
            "{'train': tensor(2.5833), 'val': tensor(2.6138)}\n",
            "{'train': tensor(2.5836), 'val': tensor(2.6161)}\n",
            "{'train': tensor(2.5773), 'val': tensor(2.6205)}\n",
            "{'train': tensor(2.5996), 'val': tensor(2.6165)}\n",
            "{'train': tensor(2.5800), 'val': tensor(2.6080)}\n",
            "{'train': tensor(2.5630), 'val': tensor(2.6121)}\n",
            "{'train': tensor(2.5865), 'val': tensor(2.6211)}\n",
            "{'train': tensor(2.5785), 'val': tensor(2.6216)}\n",
            "{'train': tensor(2.5719), 'val': tensor(2.6140)}\n",
            "{'train': tensor(2.5836), 'val': tensor(2.6264)}\n",
            "{'train': tensor(2.5861), 'val': tensor(2.6134)}\n",
            "{'train': tensor(2.5673), 'val': tensor(2.6130)}\n",
            "{'train': tensor(2.5854), 'val': tensor(2.6101)}\n",
            "{'train': tensor(2.5790), 'val': tensor(2.6212)}\n",
            "{'train': tensor(2.5688), 'val': tensor(2.6199)}\n",
            "{'train': tensor(2.5789), 'val': tensor(2.6224)}\n",
            "{'train': tensor(2.5834), 'val': tensor(2.6151)}\n",
            "{'train': tensor(2.5820), 'val': tensor(2.6346)}\n",
            "{'train': tensor(2.5895), 'val': tensor(2.6178)}\n",
            "{'train': tensor(2.5874), 'val': tensor(2.6264)}\n",
            "{'train': tensor(2.5728), 'val': tensor(2.6167)}\n",
            "{'train': tensor(2.5825), 'val': tensor(2.6082)}\n",
            "{'train': tensor(2.5974), 'val': tensor(2.6104)}\n",
            "{'train': tensor(2.5803), 'val': tensor(2.6215)}\n",
            "{'train': tensor(2.5748), 'val': tensor(2.6211)}\n",
            "{'train': tensor(2.5866), 'val': tensor(2.6329)}\n",
            "{'train': tensor(2.5798), 'val': tensor(2.6115)}\n",
            "{'train': tensor(2.5843), 'val': tensor(2.6182)}\n",
            "{'train': tensor(2.5766), 'val': tensor(2.6312)}\n",
            "{'train': tensor(2.5900), 'val': tensor(2.6163)}\n",
            "{'train': tensor(2.5742), 'val': tensor(2.6225)}\n",
            "{'train': tensor(2.5806), 'val': tensor(2.6110)}\n",
            "{'train': tensor(2.5826), 'val': tensor(2.6022)}\n",
            "{'train': tensor(2.5833), 'val': tensor(2.6042)}\n",
            "{'train': tensor(2.5871), 'val': tensor(2.6162)}\n",
            "{'train': tensor(2.5963), 'val': tensor(2.6193)}\n",
            "{'train': tensor(2.5975), 'val': tensor(2.6114)}\n",
            "{'train': tensor(2.5695), 'val': tensor(2.6021)}\n",
            "{'train': tensor(2.5743), 'val': tensor(2.6282)}\n",
            "{'train': tensor(2.5827), 'val': tensor(2.6236)}\n",
            "{'train': tensor(2.5856), 'val': tensor(2.6197)}\n",
            "{'train': tensor(2.5702), 'val': tensor(2.6283)}\n",
            "{'train': tensor(2.5917), 'val': tensor(2.6091)}\n",
            "{'train': tensor(2.5678), 'val': tensor(2.6184)}\n",
            "{'train': tensor(2.5821), 'val': tensor(2.6177)}\n",
            "{'train': tensor(2.5613), 'val': tensor(2.6145)}\n",
            "{'train': tensor(2.5714), 'val': tensor(2.6146)}\n",
            "{'train': tensor(2.5778), 'val': tensor(2.6094)}\n",
            "{'train': tensor(2.5827), 'val': tensor(2.6167)}\n",
            "{'train': tensor(2.5782), 'val': tensor(2.6132)}\n",
            "{'train': tensor(2.5791), 'val': tensor(2.6151)}\n",
            "{'train': tensor(2.5669), 'val': tensor(2.6237)}\n",
            "{'train': tensor(2.5942), 'val': tensor(2.6095)}\n",
            "{'train': tensor(2.5893), 'val': tensor(2.6148)}\n",
            "{'train': tensor(2.5752), 'val': tensor(2.6158)}\n",
            "{'train': tensor(2.5764), 'val': tensor(2.5954)}\n",
            "{'train': tensor(2.5767), 'val': tensor(2.5991)}\n",
            "{'train': tensor(2.5736), 'val': tensor(2.6237)}\n",
            "{'train': tensor(2.5858), 'val': tensor(2.6174)}\n",
            "{'train': tensor(2.5664), 'val': tensor(2.6009)}\n",
            "{'train': tensor(2.5841), 'val': tensor(2.6233)}\n",
            "{'train': tensor(2.5756), 'val': tensor(2.5965)}\n",
            "{'train': tensor(2.5827), 'val': tensor(2.6132)}\n",
            "{'train': tensor(2.5714), 'val': tensor(2.6143)}\n",
            "{'train': tensor(2.5786), 'val': tensor(2.6352)}\n",
            "{'train': tensor(2.5807), 'val': tensor(2.6051)}\n",
            "{'train': tensor(2.5874), 'val': tensor(2.6066)}\n",
            "{'train': tensor(2.5826), 'val': tensor(2.6055)}\n",
            "{'train': tensor(2.5831), 'val': tensor(2.6213)}\n",
            "{'train': tensor(2.5831), 'val': tensor(2.6012)}\n",
            "{'train': tensor(2.5789), 'val': tensor(2.6229)}\n",
            "{'train': tensor(2.5723), 'val': tensor(2.6001)}\n",
            "{'train': tensor(2.5822), 'val': tensor(2.6107)}\n",
            "{'train': tensor(2.5663), 'val': tensor(2.6098)}\n",
            "{'train': tensor(2.5662), 'val': tensor(2.5939)}\n",
            "{'train': tensor(2.5864), 'val': tensor(2.5964)}\n",
            "{'train': tensor(2.5624), 'val': tensor(2.6084)}\n",
            "{'train': tensor(2.5834), 'val': tensor(2.5890)}\n",
            "{'train': tensor(2.5702), 'val': tensor(2.6172)}\n",
            "{'train': tensor(2.5777), 'val': tensor(2.6087)}\n",
            "{'train': tensor(2.5869), 'val': tensor(2.5905)}\n",
            "{'train': tensor(2.5769), 'val': tensor(2.5962)}\n",
            "{'train': tensor(2.5582), 'val': tensor(2.6270)}\n",
            "{'train': tensor(2.5787), 'val': tensor(2.6146)}\n",
            "{'train': tensor(2.5709), 'val': tensor(2.6092)}\n",
            "{'train': tensor(2.5640), 'val': tensor(2.6085)}\n",
            "{'train': tensor(2.5609), 'val': tensor(2.6027)}\n",
            "{'train': tensor(2.5733), 'val': tensor(2.6087)}\n",
            "{'train': tensor(2.5728), 'val': tensor(2.6166)}\n",
            "{'train': tensor(2.5820), 'val': tensor(2.6093)}\n",
            "{'train': tensor(2.5678), 'val': tensor(2.6135)}\n",
            "{'train': tensor(2.5643), 'val': tensor(2.6105)}\n",
            "{'train': tensor(2.5699), 'val': tensor(2.5992)}\n",
            "{'train': tensor(2.5672), 'val': tensor(2.6022)}\n",
            "{'train': tensor(2.5844), 'val': tensor(2.5963)}\n",
            "{'train': tensor(2.5609), 'val': tensor(2.6007)}\n",
            "{'train': tensor(2.5740), 'val': tensor(2.6166)}\n",
            "{'train': tensor(2.5895), 'val': tensor(2.5967)}\n",
            "{'train': tensor(2.5804), 'val': tensor(2.6143)}\n",
            "{'train': tensor(2.5626), 'val': tensor(2.5936)}\n",
            "{'train': tensor(2.5703), 'val': tensor(2.5990)}\n",
            "{'train': tensor(2.5765), 'val': tensor(2.5933)}\n",
            "{'train': tensor(2.5687), 'val': tensor(2.6025)}\n",
            "{'train': tensor(2.5676), 'val': tensor(2.5915)}\n",
            "{'train': tensor(2.5613), 'val': tensor(2.6146)}\n",
            "{'train': tensor(2.5742), 'val': tensor(2.5937)}\n",
            "{'train': tensor(2.5631), 'val': tensor(2.6105)}\n",
            "{'train': tensor(2.5641), 'val': tensor(2.6036)}\n",
            "{'train': tensor(2.5676), 'val': tensor(2.5941)}\n",
            "{'train': tensor(2.5699), 'val': tensor(2.6128)}\n",
            "{'train': tensor(2.5721), 'val': tensor(2.5898)}\n",
            "{'train': tensor(2.5794), 'val': tensor(2.5941)}\n",
            "{'train': tensor(2.5683), 'val': tensor(2.5967)}\n",
            "{'train': tensor(2.5728), 'val': tensor(2.6011)}\n",
            "{'train': tensor(2.5656), 'val': tensor(2.6128)}\n",
            "{'train': tensor(2.5695), 'val': tensor(2.6147)}\n",
            "{'train': tensor(2.5715), 'val': tensor(2.6078)}\n",
            "{'train': tensor(2.5671), 'val': tensor(2.5966)}\n",
            "{'train': tensor(2.5604), 'val': tensor(2.6026)}\n",
            "{'train': tensor(2.5723), 'val': tensor(2.6077)}\n",
            "{'train': tensor(2.5672), 'val': tensor(2.6014)}\n",
            "{'train': tensor(2.5713), 'val': tensor(2.6034)}\n",
            "{'train': tensor(2.5737), 'val': tensor(2.5853)}\n",
            "{'train': tensor(2.5623), 'val': tensor(2.6022)}\n",
            "{'train': tensor(2.5682), 'val': tensor(2.6218)}\n",
            "{'train': tensor(2.5761), 'val': tensor(2.6058)}\n",
            "{'train': tensor(2.5742), 'val': tensor(2.6168)}\n",
            "{'train': tensor(2.5728), 'val': tensor(2.6019)}\n",
            "{'train': tensor(2.5822), 'val': tensor(2.6002)}\n",
            "{'train': tensor(2.5721), 'val': tensor(2.5975)}\n",
            "{'train': tensor(2.5879), 'val': tensor(2.6116)}\n",
            "{'train': tensor(2.5589), 'val': tensor(2.5811)}\n",
            "{'train': tensor(2.5589), 'val': tensor(2.5979)}\n",
            "{'train': tensor(2.5427), 'val': tensor(2.6065)}\n",
            "{'train': tensor(2.5665), 'val': tensor(2.6027)}\n",
            "{'train': tensor(2.5679), 'val': tensor(2.6028)}\n",
            "{'train': tensor(2.5625), 'val': tensor(2.6096)}\n",
            "{'train': tensor(2.5630), 'val': tensor(2.6016)}\n",
            "{'train': tensor(2.5654), 'val': tensor(2.6000)}\n",
            "{'train': tensor(2.5719), 'val': tensor(2.5836)}\n",
            "{'train': tensor(2.5700), 'val': tensor(2.6105)}\n",
            "{'train': tensor(2.5707), 'val': tensor(2.5937)}\n",
            "{'train': tensor(2.5592), 'val': tensor(2.6022)}\n",
            "{'train': tensor(2.5642), 'val': tensor(2.5957)}\n",
            "{'train': tensor(2.5636), 'val': tensor(2.6043)}\n",
            "{'train': tensor(2.5662), 'val': tensor(2.5906)}\n",
            "{'train': tensor(2.5696), 'val': tensor(2.5861)}\n",
            "{'train': tensor(2.5800), 'val': tensor(2.5961)}\n",
            "{'train': tensor(2.5579), 'val': tensor(2.6052)}\n",
            "{'train': tensor(2.5654), 'val': tensor(2.5910)}\n",
            "{'train': tensor(2.5476), 'val': tensor(2.5889)}\n",
            "{'train': tensor(2.5620), 'val': tensor(2.6119)}\n",
            "{'train': tensor(2.5632), 'val': tensor(2.5973)}\n",
            "{'train': tensor(2.5664), 'val': tensor(2.6102)}\n",
            "{'train': tensor(2.5642), 'val': tensor(2.6047)}\n",
            "{'train': tensor(2.5471), 'val': tensor(2.5884)}\n",
            "{'train': tensor(2.5655), 'val': tensor(2.5978)}\n",
            "{'train': tensor(2.5684), 'val': tensor(2.5825)}\n",
            "{'train': tensor(2.5693), 'val': tensor(2.6022)}\n",
            "{'train': tensor(2.5586), 'val': tensor(2.6042)}\n",
            "{'train': tensor(2.5675), 'val': tensor(2.5915)}\n",
            "{'train': tensor(2.5648), 'val': tensor(2.6127)}\n",
            "{'train': tensor(2.5678), 'val': tensor(2.5953)}\n",
            "{'train': tensor(2.5733), 'val': tensor(2.6103)}\n",
            "{'train': tensor(2.5725), 'val': tensor(2.5877)}\n",
            "{'train': tensor(2.5637), 'val': tensor(2.5963)}\n",
            "{'train': tensor(2.5741), 'val': tensor(2.6036)}\n",
            "{'train': tensor(2.5593), 'val': tensor(2.6240)}\n",
            "{'train': tensor(2.5551), 'val': tensor(2.5993)}\n",
            "{'train': tensor(2.5747), 'val': tensor(2.5953)}\n",
            "{'train': tensor(2.5648), 'val': tensor(2.6003)}\n",
            "{'train': tensor(2.5568), 'val': tensor(2.5956)}\n",
            "{'train': tensor(2.5743), 'val': tensor(2.5946)}\n",
            "{'train': tensor(2.5673), 'val': tensor(2.5858)}\n",
            "{'train': tensor(2.5738), 'val': tensor(2.5930)}\n",
            "{'train': tensor(2.5745), 'val': tensor(2.5928)}\n",
            "{'train': tensor(2.5608), 'val': tensor(2.5975)}\n",
            "{'train': tensor(2.5557), 'val': tensor(2.6021)}\n",
            "{'train': tensor(2.5635), 'val': tensor(2.5889)}\n",
            "{'train': tensor(2.5547), 'val': tensor(2.5929)}\n",
            "{'train': tensor(2.5599), 'val': tensor(2.5969)}\n",
            "{'train': tensor(2.5546), 'val': tensor(2.5977)}\n",
            "{'train': tensor(2.5689), 'val': tensor(2.5926)}\n",
            "{'train': tensor(2.5642), 'val': tensor(2.6007)}\n",
            "{'train': tensor(2.5681), 'val': tensor(2.5912)}\n",
            "{'train': tensor(2.5627), 'val': tensor(2.5719)}\n",
            "{'train': tensor(2.5707), 'val': tensor(2.5886)}\n",
            "{'train': tensor(2.5492), 'val': tensor(2.5844)}\n",
            "{'train': tensor(2.5484), 'val': tensor(2.5901)}\n",
            "{'train': tensor(2.5481), 'val': tensor(2.5851)}\n",
            "{'train': tensor(2.5601), 'val': tensor(2.5882)}\n",
            "{'train': tensor(2.5572), 'val': tensor(2.5893)}\n",
            "{'train': tensor(2.5507), 'val': tensor(2.5767)}\n",
            "{'train': tensor(2.5547), 'val': tensor(2.6031)}\n",
            "{'train': tensor(2.5697), 'val': tensor(2.5762)}\n",
            "{'train': tensor(2.5510), 'val': tensor(2.5945)}\n",
            "{'train': tensor(2.5673), 'val': tensor(2.5885)}\n",
            "{'train': tensor(2.5474), 'val': tensor(2.5975)}\n",
            "{'train': tensor(2.5567), 'val': tensor(2.5932)}\n",
            "{'train': tensor(2.5687), 'val': tensor(2.5792)}\n",
            "{'train': tensor(2.5606), 'val': tensor(2.6129)}\n",
            "{'train': tensor(2.5642), 'val': tensor(2.6033)}\n",
            "{'train': tensor(2.5753), 'val': tensor(2.5966)}\n",
            "{'train': tensor(2.5693), 'val': tensor(2.5853)}\n",
            "{'train': tensor(2.5501), 'val': tensor(2.5912)}\n",
            "{'train': tensor(2.5585), 'val': tensor(2.5864)}\n",
            "{'train': tensor(2.5537), 'val': tensor(2.5867)}\n",
            "{'train': tensor(2.5585), 'val': tensor(2.5853)}\n",
            "{'train': tensor(2.5591), 'val': tensor(2.6028)}\n",
            "{'train': tensor(2.5519), 'val': tensor(2.5887)}\n",
            "{'train': tensor(2.5632), 'val': tensor(2.5832)}\n",
            "{'train': tensor(2.5698), 'val': tensor(2.5981)}\n",
            "{'train': tensor(2.5619), 'val': tensor(2.5941)}\n",
            "{'train': tensor(2.5327), 'val': tensor(2.5905)}\n",
            "{'train': tensor(2.5605), 'val': tensor(2.5770)}\n",
            "{'train': tensor(2.5589), 'val': tensor(2.5977)}\n",
            "{'train': tensor(2.5577), 'val': tensor(2.5911)}\n",
            "{'train': tensor(2.5536), 'val': tensor(2.5763)}\n",
            "{'train': tensor(2.5617), 'val': tensor(2.5931)}\n",
            "{'train': tensor(2.5496), 'val': tensor(2.5835)}\n",
            "{'train': tensor(2.5586), 'val': tensor(2.5852)}\n",
            "{'train': tensor(2.5567), 'val': tensor(2.5782)}\n",
            "{'train': tensor(2.5526), 'val': tensor(2.5819)}\n",
            "{'train': tensor(2.5533), 'val': tensor(2.5776)}\n",
            "{'train': tensor(2.5637), 'val': tensor(2.5911)}\n",
            "{'train': tensor(2.5659), 'val': tensor(2.5872)}\n",
            "{'train': tensor(2.5585), 'val': tensor(2.5957)}\n",
            "{'train': tensor(2.5648), 'val': tensor(2.5975)}\n",
            "{'train': tensor(2.5479), 'val': tensor(2.5947)}\n",
            "{'train': tensor(2.5428), 'val': tensor(2.5934)}\n",
            "{'train': tensor(2.5519), 'val': tensor(2.5938)}\n",
            "{'train': tensor(2.5683), 'val': tensor(2.5871)}\n",
            "{'train': tensor(2.5497), 'val': tensor(2.5867)}\n",
            "{'train': tensor(2.5563), 'val': tensor(2.5852)}\n",
            "{'train': tensor(2.5480), 'val': tensor(2.5913)}\n",
            "{'train': tensor(2.5578), 'val': tensor(2.5952)}\n",
            "{'train': tensor(2.5650), 'val': tensor(2.5742)}\n",
            "{'train': tensor(2.5657), 'val': tensor(2.5854)}\n",
            "{'train': tensor(2.5512), 'val': tensor(2.6026)}\n",
            "{'train': tensor(2.5532), 'val': tensor(2.6066)}\n",
            "{'train': tensor(2.5535), 'val': tensor(2.5797)}\n",
            "{'train': tensor(2.5480), 'val': tensor(2.5946)}\n",
            "{'train': tensor(2.5693), 'val': tensor(2.5876)}\n",
            "{'train': tensor(2.5576), 'val': tensor(2.5859)}\n",
            "{'train': tensor(2.5566), 'val': tensor(2.5895)}\n",
            "{'train': tensor(2.5405), 'val': tensor(2.5903)}\n",
            "{'train': tensor(2.5584), 'val': tensor(2.5818)}\n",
            "{'train': tensor(2.5552), 'val': tensor(2.5888)}\n",
            "{'train': tensor(2.5552), 'val': tensor(2.5976)}\n",
            "{'train': tensor(2.5517), 'val': tensor(2.5817)}\n",
            "{'train': tensor(2.5522), 'val': tensor(2.5878)}\n",
            "{'train': tensor(2.5589), 'val': tensor(2.5886)}\n",
            "{'train': tensor(2.5430), 'val': tensor(2.5741)}\n",
            "{'train': tensor(2.5534), 'val': tensor(2.5870)}\n",
            "{'train': tensor(2.5385), 'val': tensor(2.5877)}\n",
            "{'train': tensor(2.5550), 'val': tensor(2.5796)}\n",
            "{'train': tensor(2.5561), 'val': tensor(2.5816)}\n",
            "{'train': tensor(2.5380), 'val': tensor(2.5873)}\n",
            "{'train': tensor(2.5532), 'val': tensor(2.5849)}\n",
            "{'train': tensor(2.5561), 'val': tensor(2.5947)}\n",
            "{'train': tensor(2.5472), 'val': tensor(2.5769)}\n",
            "{'train': tensor(2.5533), 'val': tensor(2.5984)}\n",
            "{'train': tensor(2.5518), 'val': tensor(2.5852)}\n",
            "{'train': tensor(2.5561), 'val': tensor(2.5825)}\n",
            "{'train': tensor(2.5573), 'val': tensor(2.5845)}\n",
            "{'train': tensor(2.5469), 'val': tensor(2.5906)}\n",
            "{'train': tensor(2.5490), 'val': tensor(2.5881)}\n",
            "{'train': tensor(2.5298), 'val': tensor(2.5919)}\n",
            "{'train': tensor(2.5490), 'val': tensor(2.5737)}\n",
            "{'train': tensor(2.5698), 'val': tensor(2.5840)}\n",
            "{'train': tensor(2.5522), 'val': tensor(2.5822)}\n",
            "{'train': tensor(2.5567), 'val': tensor(2.5707)}\n",
            "{'train': tensor(2.5484), 'val': tensor(2.5923)}\n",
            "{'train': tensor(2.5610), 'val': tensor(2.5810)}\n",
            "{'train': tensor(2.5524), 'val': tensor(2.5755)}\n",
            "{'train': tensor(2.5493), 'val': tensor(2.5835)}\n",
            "{'train': tensor(2.5336), 'val': tensor(2.5961)}\n",
            "{'train': tensor(2.5585), 'val': tensor(2.5854)}\n",
            "{'train': tensor(2.5570), 'val': tensor(2.5818)}\n",
            "{'train': tensor(2.5537), 'val': tensor(2.5918)}\n",
            "{'train': tensor(2.5530), 'val': tensor(2.5887)}\n",
            "{'train': tensor(2.5609), 'val': tensor(2.5769)}\n",
            "{'train': tensor(2.5462), 'val': tensor(2.5846)}\n",
            "{'train': tensor(2.5389), 'val': tensor(2.5899)}\n",
            "{'train': tensor(2.5586), 'val': tensor(2.5684)}\n",
            "{'train': tensor(2.5426), 'val': tensor(2.5747)}\n",
            "{'train': tensor(2.5308), 'val': tensor(2.5685)}\n",
            "{'train': tensor(2.5521), 'val': tensor(2.5716)}\n",
            "{'train': tensor(2.5579), 'val': tensor(2.5885)}\n",
            "{'train': tensor(2.5595), 'val': tensor(2.5824)}\n",
            "{'train': tensor(2.5497), 'val': tensor(2.5826)}\n",
            "{'train': tensor(2.5397), 'val': tensor(2.5808)}\n",
            "{'train': tensor(2.5423), 'val': tensor(2.5695)}\n",
            "{'train': tensor(2.5474), 'val': tensor(2.5870)}\n",
            "{'train': tensor(2.5439), 'val': tensor(2.5903)}\n",
            "{'train': tensor(2.5459), 'val': tensor(2.5860)}\n",
            "{'train': tensor(2.5450), 'val': tensor(2.5814)}\n",
            "{'train': tensor(2.5560), 'val': tensor(2.5688)}\n",
            "{'train': tensor(2.5535), 'val': tensor(2.5833)}\n",
            "{'train': tensor(2.5545), 'val': tensor(2.5790)}\n",
            "{'train': tensor(2.5543), 'val': tensor(2.5894)}\n",
            "{'train': tensor(2.5486), 'val': tensor(2.5778)}\n",
            "{'train': tensor(2.5481), 'val': tensor(2.5810)}\n",
            "{'train': tensor(2.5383), 'val': tensor(2.5843)}\n",
            "{'train': tensor(2.5440), 'val': tensor(2.5927)}\n",
            "{'train': tensor(2.5419), 'val': tensor(2.5790)}\n",
            "{'train': tensor(2.5559), 'val': tensor(2.5795)}\n",
            "{'train': tensor(2.5605), 'val': tensor(2.5867)}\n",
            "{'train': tensor(2.5421), 'val': tensor(2.5804)}\n",
            "{'train': tensor(2.5433), 'val': tensor(2.5689)}\n",
            "{'train': tensor(2.5491), 'val': tensor(2.5802)}\n",
            "{'train': tensor(2.5457), 'val': tensor(2.5967)}\n",
            "{'train': tensor(2.5564), 'val': tensor(2.5785)}\n",
            "{'train': tensor(2.5429), 'val': tensor(2.5891)}\n",
            "{'train': tensor(2.5528), 'val': tensor(2.5662)}\n",
            "{'train': tensor(2.5491), 'val': tensor(2.5714)}\n",
            "{'train': tensor(2.5402), 'val': tensor(2.5878)}\n",
            "{'train': tensor(2.5466), 'val': tensor(2.5878)}\n",
            "{'train': tensor(2.5452), 'val': tensor(2.5738)}\n",
            "{'train': tensor(2.5404), 'val': tensor(2.5799)}\n",
            "{'train': tensor(2.5449), 'val': tensor(2.5813)}\n",
            "{'train': tensor(2.5510), 'val': tensor(2.5585)}\n",
            "{'train': tensor(2.5533), 'val': tensor(2.5930)}\n",
            "{'train': tensor(2.5622), 'val': tensor(2.5820)}\n",
            "{'train': tensor(2.5431), 'val': tensor(2.5900)}\n",
            "{'train': tensor(2.5553), 'val': tensor(2.5732)}\n",
            "{'train': tensor(2.5513), 'val': tensor(2.5814)}\n",
            "{'train': tensor(2.5508), 'val': tensor(2.5729)}\n",
            "{'train': tensor(2.5538), 'val': tensor(2.5750)}\n",
            "{'train': tensor(2.5437), 'val': tensor(2.5697)}\n",
            "{'train': tensor(2.5414), 'val': tensor(2.5558)}\n",
            "{'train': tensor(2.5488), 'val': tensor(2.5758)}\n",
            "{'train': tensor(2.5471), 'val': tensor(2.5826)}\n",
            "{'train': tensor(2.5487), 'val': tensor(2.5741)}\n",
            "{'train': tensor(2.5541), 'val': tensor(2.5802)}\n",
            "{'train': tensor(2.5531), 'val': tensor(2.5872)}\n",
            "{'train': tensor(2.5504), 'val': tensor(2.5681)}\n",
            "{'train': tensor(2.5331), 'val': tensor(2.5710)}\n",
            "{'train': tensor(2.5326), 'val': tensor(2.5601)}\n",
            "{'train': tensor(2.5378), 'val': tensor(2.5786)}\n",
            "{'train': tensor(2.5701), 'val': tensor(2.5770)}\n",
            "{'train': tensor(2.5463), 'val': tensor(2.5690)}\n",
            "{'train': tensor(2.5503), 'val': tensor(2.5806)}\n",
            "{'train': tensor(2.5717), 'val': tensor(2.5936)}\n",
            "{'train': tensor(2.5388), 'val': tensor(2.5767)}\n",
            "{'train': tensor(2.5493), 'val': tensor(2.5876)}\n",
            "{'train': tensor(2.5502), 'val': tensor(2.5642)}\n",
            "{'train': tensor(2.5578), 'val': tensor(2.5791)}\n",
            "{'train': tensor(2.5478), 'val': tensor(2.5729)}\n",
            "{'train': tensor(2.5603), 'val': tensor(2.5895)}\n",
            "{'train': tensor(2.5456), 'val': tensor(2.5888)}\n",
            "{'train': tensor(2.5476), 'val': tensor(2.5692)}\n",
            "{'train': tensor(2.5588), 'val': tensor(2.5816)}\n",
            "{'train': tensor(2.5419), 'val': tensor(2.5698)}\n",
            "{'train': tensor(2.5597), 'val': tensor(2.5811)}\n",
            "{'train': tensor(2.5600), 'val': tensor(2.5700)}\n",
            "{'train': tensor(2.5463), 'val': tensor(2.5695)}\n",
            "{'train': tensor(2.5520), 'val': tensor(2.5645)}\n",
            "{'train': tensor(2.5437), 'val': tensor(2.5699)}\n",
            "{'train': tensor(2.5406), 'val': tensor(2.5888)}\n",
            "{'train': tensor(2.5547), 'val': tensor(2.5883)}\n",
            "{'train': tensor(2.5578), 'val': tensor(2.5694)}\n",
            "{'train': tensor(2.5630), 'val': tensor(2.5678)}\n",
            "{'train': tensor(2.5370), 'val': tensor(2.5788)}\n",
            "{'train': tensor(2.5462), 'val': tensor(2.5745)}\n",
            "{'train': tensor(2.5506), 'val': tensor(2.5750)}\n",
            "{'train': tensor(2.5513), 'val': tensor(2.5790)}\n",
            "{'train': tensor(2.5477), 'val': tensor(2.5744)}\n",
            "{'train': tensor(2.5351), 'val': tensor(2.5763)}\n",
            "{'train': tensor(2.5447), 'val': tensor(2.5696)}\n",
            "{'train': tensor(2.5456), 'val': tensor(2.5792)}\n",
            "{'train': tensor(2.5544), 'val': tensor(2.5733)}\n",
            "{'train': tensor(2.5494), 'val': tensor(2.5794)}\n",
            "{'train': tensor(2.5535), 'val': tensor(2.5834)}\n",
            "{'train': tensor(2.5646), 'val': tensor(2.5872)}\n",
            "{'train': tensor(2.5535), 'val': tensor(2.5724)}\n",
            "{'train': tensor(2.5560), 'val': tensor(2.5694)}\n",
            "{'train': tensor(2.5572), 'val': tensor(2.5761)}\n",
            "{'train': tensor(2.5481), 'val': tensor(2.6023)}\n",
            "{'train': tensor(2.5436), 'val': tensor(2.5718)}\n",
            "{'train': tensor(2.5466), 'val': tensor(2.5705)}\n",
            "{'train': tensor(2.5422), 'val': tensor(2.6026)}\n",
            "{'train': tensor(2.5518), 'val': tensor(2.5768)}\n",
            "{'train': tensor(2.5530), 'val': tensor(2.5747)}\n",
            "{'train': tensor(2.5539), 'val': tensor(2.5789)}\n",
            "{'train': tensor(2.5460), 'val': tensor(2.5794)}\n",
            "{'train': tensor(2.5363), 'val': tensor(2.5776)}\n",
            "{'train': tensor(2.5366), 'val': tensor(2.5849)}\n",
            "{'train': tensor(2.5491), 'val': tensor(2.5681)}\n",
            "{'train': tensor(2.5520), 'val': tensor(2.5695)}\n",
            "{'train': tensor(2.5447), 'val': tensor(2.5821)}\n",
            "{'train': tensor(2.5500), 'val': tensor(2.5777)}\n",
            "{'train': tensor(2.5572), 'val': tensor(2.5664)}\n",
            "{'train': tensor(2.5504), 'val': tensor(2.5630)}\n",
            "{'train': tensor(2.5267), 'val': tensor(2.5879)}\n",
            "{'train': tensor(2.5385), 'val': tensor(2.5755)}\n",
            "{'train': tensor(2.5476), 'val': tensor(2.5774)}\n",
            "{'train': tensor(2.5366), 'val': tensor(2.5850)}\n",
            "{'train': tensor(2.5476), 'val': tensor(2.5758)}\n",
            "{'train': tensor(2.5412), 'val': tensor(2.5862)}\n",
            "{'train': tensor(2.5504), 'val': tensor(2.5794)}\n",
            "{'train': tensor(2.5449), 'val': tensor(2.5795)}\n",
            "{'train': tensor(2.5471), 'val': tensor(2.5764)}\n",
            "{'train': tensor(2.5517), 'val': tensor(2.5803)}\n",
            "{'train': tensor(2.5339), 'val': tensor(2.5684)}\n",
            "{'train': tensor(2.5307), 'val': tensor(2.5788)}\n",
            "{'train': tensor(2.5471), 'val': tensor(2.5713)}\n",
            "{'train': tensor(2.5430), 'val': tensor(2.5752)}\n",
            "{'train': tensor(2.5481), 'val': tensor(2.5758)}\n",
            "{'train': tensor(2.5520), 'val': tensor(2.5732)}\n",
            "{'train': tensor(2.5338), 'val': tensor(2.5808)}\n",
            "{'train': tensor(2.5424), 'val': tensor(2.5759)}\n",
            "{'train': tensor(2.5360), 'val': tensor(2.5641)}\n",
            "{'train': tensor(2.5410), 'val': tensor(2.5759)}\n",
            "{'train': tensor(2.5350), 'val': tensor(2.5697)}\n",
            "{'train': tensor(2.5381), 'val': tensor(2.5873)}\n",
            "{'train': tensor(2.5503), 'val': tensor(2.5778)}\n",
            "{'train': tensor(2.5448), 'val': tensor(2.5836)}\n",
            "{'train': tensor(2.5471), 'val': tensor(2.5880)}\n",
            "{'train': tensor(2.5499), 'val': tensor(2.5685)}\n",
            "{'train': tensor(2.5286), 'val': tensor(2.5739)}\n",
            "{'train': tensor(2.5417), 'val': tensor(2.5724)}\n",
            "{'train': tensor(2.5596), 'val': tensor(2.5701)}\n",
            "{'train': tensor(2.5348), 'val': tensor(2.5847)}\n",
            "{'train': tensor(2.5361), 'val': tensor(2.5724)}\n",
            "{'train': tensor(2.5562), 'val': tensor(2.5601)}\n",
            "{'train': tensor(2.5373), 'val': tensor(2.5815)}\n",
            "{'train': tensor(2.5321), 'val': tensor(2.5779)}\n",
            "{'train': tensor(2.5397), 'val': tensor(2.5677)}\n",
            "{'train': tensor(2.5261), 'val': tensor(2.5712)}\n",
            "{'train': tensor(2.5378), 'val': tensor(2.5790)}\n",
            "{'train': tensor(2.5416), 'val': tensor(2.5746)}\n",
            "{'train': tensor(2.5198), 'val': tensor(2.5664)}\n",
            "{'train': tensor(2.5358), 'val': tensor(2.5798)}\n",
            "{'train': tensor(2.5337), 'val': tensor(2.5636)}\n",
            "{'train': tensor(2.5433), 'val': tensor(2.5769)}\n",
            "{'train': tensor(2.5455), 'val': tensor(2.5797)}\n",
            "{'train': tensor(2.5332), 'val': tensor(2.5844)}\n",
            "{'train': tensor(2.5427), 'val': tensor(2.5632)}\n",
            "{'train': tensor(2.5463), 'val': tensor(2.5830)}\n",
            "{'train': tensor(2.5323), 'val': tensor(2.5619)}\n",
            "{'train': tensor(2.5457), 'val': tensor(2.5709)}\n",
            "{'train': tensor(2.5373), 'val': tensor(2.5696)}\n",
            "{'train': tensor(2.5474), 'val': tensor(2.5790)}\n",
            "{'train': tensor(2.5433), 'val': tensor(2.5808)}\n",
            "{'train': tensor(2.5392), 'val': tensor(2.5617)}\n",
            "{'train': tensor(2.5318), 'val': tensor(2.5644)}\n",
            "{'train': tensor(2.5358), 'val': tensor(2.5790)}\n",
            "{'train': tensor(2.5517), 'val': tensor(2.5669)}\n",
            "{'train': tensor(2.5527), 'val': tensor(2.5705)}\n",
            "{'train': tensor(2.5394), 'val': tensor(2.5777)}\n",
            "{'train': tensor(2.5471), 'val': tensor(2.5714)}\n",
            "{'train': tensor(2.5389), 'val': tensor(2.5782)}\n",
            "{'train': tensor(2.5506), 'val': tensor(2.5607)}\n",
            "{'train': tensor(2.5470), 'val': tensor(2.5727)}\n",
            "{'train': tensor(2.5460), 'val': tensor(2.5735)}\n",
            "{'train': tensor(2.5284), 'val': tensor(2.5783)}\n",
            "{'train': tensor(2.5338), 'val': tensor(2.5896)}\n",
            "{'train': tensor(2.5300), 'val': tensor(2.5800)}\n",
            "{'train': tensor(2.5348), 'val': tensor(2.5627)}\n",
            "{'train': tensor(2.5353), 'val': tensor(2.5745)}\n",
            "{'train': tensor(2.5483), 'val': tensor(2.5770)}\n",
            "{'train': tensor(2.5294), 'val': tensor(2.5865)}\n",
            "{'train': tensor(2.5341), 'val': tensor(2.5685)}\n",
            "{'train': tensor(2.5407), 'val': tensor(2.5556)}\n",
            "{'train': tensor(2.5375), 'val': tensor(2.5624)}\n",
            "{'train': tensor(2.5388), 'val': tensor(2.5828)}\n",
            "{'train': tensor(2.5362), 'val': tensor(2.5789)}\n",
            "{'train': tensor(2.5458), 'val': tensor(2.5744)}\n",
            "{'train': tensor(2.5345), 'val': tensor(2.5707)}\n",
            "{'train': tensor(2.5448), 'val': tensor(2.5626)}\n",
            "{'train': tensor(2.5439), 'val': tensor(2.5825)}\n",
            "{'train': tensor(2.5359), 'val': tensor(2.5771)}\n",
            "{'train': tensor(2.5279), 'val': tensor(2.5466)}\n",
            "{'train': tensor(2.5358), 'val': tensor(2.5784)}\n",
            "{'train': tensor(2.5407), 'val': tensor(2.5718)}\n",
            "{'train': tensor(2.5427), 'val': tensor(2.5703)}\n",
            "{'train': tensor(2.5378), 'val': tensor(2.5860)}\n",
            "{'train': tensor(2.5527), 'val': tensor(2.5743)}\n",
            "{'train': tensor(2.5352), 'val': tensor(2.5626)}\n",
            "{'train': tensor(2.5366), 'val': tensor(2.5726)}\n",
            "{'train': tensor(2.5359), 'val': tensor(2.5595)}\n",
            "{'train': tensor(2.5257), 'val': tensor(2.5624)}\n",
            "{'train': tensor(2.5494), 'val': tensor(2.5616)}\n",
            "{'train': tensor(2.5391), 'val': tensor(2.5766)}\n",
            "{'train': tensor(2.5488), 'val': tensor(2.5533)}\n",
            "{'train': tensor(2.5487), 'val': tensor(2.5804)}\n",
            "{'train': tensor(2.5281), 'val': tensor(2.5863)}\n",
            "{'train': tensor(2.5336), 'val': tensor(2.5800)}\n",
            "{'train': tensor(2.5356), 'val': tensor(2.5652)}\n",
            "{'train': tensor(2.5348), 'val': tensor(2.5474)}\n",
            "{'train': tensor(2.5440), 'val': tensor(2.5704)}\n",
            "{'train': tensor(2.5545), 'val': tensor(2.5587)}\n",
            "{'train': tensor(2.5326), 'val': tensor(2.5559)}\n",
            "{'train': tensor(2.5450), 'val': tensor(2.5618)}\n",
            "{'train': tensor(2.5266), 'val': tensor(2.5706)}\n",
            "{'train': tensor(2.5328), 'val': tensor(2.5590)}\n",
            "{'train': tensor(2.5438), 'val': tensor(2.5693)}\n",
            "{'train': tensor(2.5338), 'val': tensor(2.5708)}\n",
            "{'train': tensor(2.5279), 'val': tensor(2.5733)}\n",
            "{'train': tensor(2.5352), 'val': tensor(2.5758)}\n",
            "{'train': tensor(2.5344), 'val': tensor(2.5677)}\n",
            "{'train': tensor(2.5527), 'val': tensor(2.5703)}\n",
            "{'train': tensor(2.5427), 'val': tensor(2.5700)}\n",
            "{'train': tensor(2.5439), 'val': tensor(2.5801)}\n",
            "{'train': tensor(2.5388), 'val': tensor(2.5660)}\n",
            "{'train': tensor(2.5418), 'val': tensor(2.5752)}\n",
            "{'train': tensor(2.5280), 'val': tensor(2.5664)}\n",
            "{'train': tensor(2.5404), 'val': tensor(2.5782)}\n",
            "{'train': tensor(2.5249), 'val': tensor(2.5680)}\n",
            "{'train': tensor(2.5266), 'val': tensor(2.5722)}\n",
            "{'train': tensor(2.5375), 'val': tensor(2.5657)}\n",
            "{'train': tensor(2.5436), 'val': tensor(2.5765)}\n",
            "{'train': tensor(2.5455), 'val': tensor(2.5583)}\n",
            "{'train': tensor(2.5404), 'val': tensor(2.5582)}\n",
            "{'train': tensor(2.5293), 'val': tensor(2.5826)}\n",
            "{'train': tensor(2.5297), 'val': tensor(2.5804)}\n",
            "{'train': tensor(2.5342), 'val': tensor(2.5791)}\n",
            "{'train': tensor(2.5306), 'val': tensor(2.5640)}\n",
            "{'train': tensor(2.5277), 'val': tensor(2.5648)}\n",
            "{'train': tensor(2.5266), 'val': tensor(2.5576)}\n",
            "{'train': tensor(2.5412), 'val': tensor(2.5572)}\n",
            "{'train': tensor(2.5383), 'val': tensor(2.5837)}\n",
            "{'train': tensor(2.5387), 'val': tensor(2.5724)}\n",
            "{'train': tensor(2.5346), 'val': tensor(2.5726)}\n",
            "{'train': tensor(2.5426), 'val': tensor(2.5625)}\n",
            "{'train': tensor(2.5263), 'val': tensor(2.5621)}\n",
            "{'train': tensor(2.5450), 'val': tensor(2.5562)}\n",
            "{'train': tensor(2.5327), 'val': tensor(2.5765)}\n",
            "{'train': tensor(2.5329), 'val': tensor(2.5767)}\n",
            "{'train': tensor(2.5399), 'val': tensor(2.5626)}\n",
            "{'train': tensor(2.5304), 'val': tensor(2.5521)}\n",
            "{'train': tensor(2.5371), 'val': tensor(2.5776)}\n",
            "{'train': tensor(2.5367), 'val': tensor(2.5669)}\n",
            "{'train': tensor(2.5372), 'val': tensor(2.5699)}\n",
            "{'train': tensor(2.5295), 'val': tensor(2.5727)}\n",
            "{'train': tensor(2.5335), 'val': tensor(2.5708)}\n",
            "{'train': tensor(2.5347), 'val': tensor(2.5657)}\n",
            "{'train': tensor(2.5345), 'val': tensor(2.5515)}\n",
            "{'train': tensor(2.5499), 'val': tensor(2.5659)}\n",
            "{'train': tensor(2.5242), 'val': tensor(2.5554)}\n",
            "{'train': tensor(2.5336), 'val': tensor(2.5733)}\n",
            "{'train': tensor(2.5265), 'val': tensor(2.5512)}\n",
            "{'train': tensor(2.5250), 'val': tensor(2.5772)}\n",
            "{'train': tensor(2.5522), 'val': tensor(2.5676)}\n",
            "{'train': tensor(2.5354), 'val': tensor(2.5602)}\n",
            "{'train': tensor(2.5287), 'val': tensor(2.5566)}\n",
            "{'train': tensor(2.5253), 'val': tensor(2.5660)}\n",
            "{'train': tensor(2.5460), 'val': tensor(2.5706)}\n",
            "{'train': tensor(2.5375), 'val': tensor(2.5511)}\n",
            "{'train': tensor(2.5373), 'val': tensor(2.5758)}\n",
            "{'train': tensor(2.5186), 'val': tensor(2.5582)}\n",
            "{'train': tensor(2.5222), 'val': tensor(2.5698)}\n",
            "{'train': tensor(2.5411), 'val': tensor(2.5690)}\n",
            "{'train': tensor(2.5282), 'val': tensor(2.5709)}\n",
            "{'train': tensor(2.5557), 'val': tensor(2.5563)}\n",
            "{'train': tensor(2.5271), 'val': tensor(2.5688)}\n",
            "{'train': tensor(2.5227), 'val': tensor(2.5566)}\n",
            "{'train': tensor(2.5511), 'val': tensor(2.5791)}\n",
            "{'train': tensor(2.5286), 'val': tensor(2.5561)}\n",
            "{'train': tensor(2.5403), 'val': tensor(2.5530)}\n",
            "{'train': tensor(2.5356), 'val': tensor(2.5587)}\n",
            "{'train': tensor(2.5482), 'val': tensor(2.5786)}\n",
            "{'train': tensor(2.5364), 'val': tensor(2.5662)}\n",
            "{'train': tensor(2.5386), 'val': tensor(2.5689)}\n",
            "{'train': tensor(2.5330), 'val': tensor(2.5545)}\n",
            "{'train': tensor(2.5319), 'val': tensor(2.5669)}\n",
            "{'train': tensor(2.5432), 'val': tensor(2.5739)}\n",
            "{'train': tensor(2.5441), 'val': tensor(2.5630)}\n",
            "{'train': tensor(2.5341), 'val': tensor(2.5733)}\n",
            "{'train': tensor(2.5285), 'val': tensor(2.5611)}\n",
            "{'train': tensor(2.5271), 'val': tensor(2.5573)}\n",
            "{'train': tensor(2.5273), 'val': tensor(2.5747)}\n",
            "{'train': tensor(2.5443), 'val': tensor(2.5683)}\n",
            "{'train': tensor(2.5405), 'val': tensor(2.5576)}\n",
            "{'train': tensor(2.5260), 'val': tensor(2.5559)}\n",
            "{'train': tensor(2.5444), 'val': tensor(2.5732)}\n",
            "{'train': tensor(2.5233), 'val': tensor(2.5549)}\n",
            "{'train': tensor(2.5229), 'val': tensor(2.5618)}\n",
            "{'train': tensor(2.5295), 'val': tensor(2.5662)}\n",
            "{'train': tensor(2.5415), 'val': tensor(2.5460)}\n",
            "{'train': tensor(2.5362), 'val': tensor(2.5646)}\n",
            "{'train': tensor(2.5344), 'val': tensor(2.5584)}\n",
            "{'train': tensor(2.5153), 'val': tensor(2.5720)}\n",
            "{'train': tensor(2.5380), 'val': tensor(2.5598)}\n",
            "{'train': tensor(2.5314), 'val': tensor(2.5756)}\n",
            "{'train': tensor(2.5343), 'val': tensor(2.5644)}\n",
            "{'train': tensor(2.5463), 'val': tensor(2.5621)}\n",
            "{'train': tensor(2.5420), 'val': tensor(2.5575)}\n",
            "{'train': tensor(2.5139), 'val': tensor(2.5624)}\n",
            "{'train': tensor(2.5339), 'val': tensor(2.5504)}\n",
            "{'train': tensor(2.5426), 'val': tensor(2.5559)}\n",
            "{'train': tensor(2.5463), 'val': tensor(2.5541)}\n",
            "{'train': tensor(2.5278), 'val': tensor(2.5537)}\n",
            "{'train': tensor(2.5391), 'val': tensor(2.5625)}\n",
            "{'train': tensor(2.5361), 'val': tensor(2.5604)}\n",
            "{'train': tensor(2.5293), 'val': tensor(2.5510)}\n",
            "{'train': tensor(2.5452), 'val': tensor(2.5582)}\n",
            "{'train': tensor(2.5325), 'val': tensor(2.5723)}\n",
            "{'train': tensor(2.5309), 'val': tensor(2.5596)}\n",
            "{'train': tensor(2.5380), 'val': tensor(2.5596)}\n",
            "{'train': tensor(2.5449), 'val': tensor(2.5553)}\n",
            "{'train': tensor(2.5405), 'val': tensor(2.5628)}\n",
            "{'train': tensor(2.5233), 'val': tensor(2.5509)}\n",
            "{'train': tensor(2.5270), 'val': tensor(2.5647)}\n",
            "{'train': tensor(2.5368), 'val': tensor(2.5696)}\n",
            "{'train': tensor(2.5557), 'val': tensor(2.5605)}\n",
            "{'train': tensor(2.5268), 'val': tensor(2.5645)}\n",
            "{'train': tensor(2.5287), 'val': tensor(2.5659)}\n",
            "{'train': tensor(2.5381), 'val': tensor(2.5552)}\n",
            "{'train': tensor(2.5371), 'val': tensor(2.5672)}\n",
            "{'train': tensor(2.5293), 'val': tensor(2.5414)}\n",
            "{'train': tensor(2.5269), 'val': tensor(2.5734)}\n",
            "{'train': tensor(2.5273), 'val': tensor(2.5570)}\n",
            "{'train': tensor(2.5351), 'val': tensor(2.5538)}\n",
            "{'train': tensor(2.5336), 'val': tensor(2.5533)}\n",
            "{'train': tensor(2.5444), 'val': tensor(2.5590)}\n",
            "{'train': tensor(2.5476), 'val': tensor(2.5530)}\n",
            "{'train': tensor(2.5314), 'val': tensor(2.5587)}\n",
            "{'train': tensor(2.5174), 'val': tensor(2.5777)}\n",
            "{'train': tensor(2.5256), 'val': tensor(2.5821)}\n",
            "{'train': tensor(2.5294), 'val': tensor(2.5568)}\n",
            "{'train': tensor(2.5302), 'val': tensor(2.5696)}\n",
            "{'train': tensor(2.5368), 'val': tensor(2.5313)}\n",
            "{'train': tensor(2.5311), 'val': tensor(2.5701)}\n",
            "{'train': tensor(2.5413), 'val': tensor(2.5651)}\n",
            "{'train': tensor(2.5304), 'val': tensor(2.5679)}\n",
            "{'train': tensor(2.5161), 'val': tensor(2.5744)}\n",
            "{'train': tensor(2.5245), 'val': tensor(2.5595)}\n",
            "{'train': tensor(2.5310), 'val': tensor(2.5432)}\n",
            "{'train': tensor(2.5326), 'val': tensor(2.5587)}\n",
            "{'train': tensor(2.5264), 'val': tensor(2.5600)}\n",
            "{'train': tensor(2.5358), 'val': tensor(2.5648)}\n",
            "{'train': tensor(2.5427), 'val': tensor(2.5627)}\n",
            "{'train': tensor(2.5229), 'val': tensor(2.5613)}\n",
            "{'train': tensor(2.5258), 'val': tensor(2.5607)}\n",
            "{'train': tensor(2.5319), 'val': tensor(2.5796)}\n",
            "{'train': tensor(2.5292), 'val': tensor(2.5594)}\n",
            "{'train': tensor(2.5202), 'val': tensor(2.5539)}\n",
            "{'train': tensor(2.5391), 'val': tensor(2.5637)}\n",
            "{'train': tensor(2.5396), 'val': tensor(2.5554)}\n",
            "{'train': tensor(2.5223), 'val': tensor(2.5617)}\n",
            "{'train': tensor(2.5394), 'val': tensor(2.5685)}\n",
            "{'train': tensor(2.5317), 'val': tensor(2.5524)}\n",
            "{'train': tensor(2.5354), 'val': tensor(2.5584)}\n",
            "{'train': tensor(2.5216), 'val': tensor(2.5680)}\n",
            "{'train': tensor(2.5087), 'val': tensor(2.5611)}\n",
            "{'train': tensor(2.5236), 'val': tensor(2.5546)}\n",
            "{'train': tensor(2.5300), 'val': tensor(2.5696)}\n",
            "{'train': tensor(2.5119), 'val': tensor(2.5650)}\n",
            "{'train': tensor(2.5239), 'val': tensor(2.5585)}\n",
            "{'train': tensor(2.5195), 'val': tensor(2.5598)}\n",
            "{'train': tensor(2.5220), 'val': tensor(2.5586)}\n",
            "{'train': tensor(2.5232), 'val': tensor(2.5516)}\n",
            "{'train': tensor(2.5275), 'val': tensor(2.5598)}\n",
            "{'train': tensor(2.5375), 'val': tensor(2.5696)}\n",
            "{'train': tensor(2.5173), 'val': tensor(2.5614)}\n",
            "{'train': tensor(2.5557), 'val': tensor(2.5628)}\n",
            "{'train': tensor(2.5264), 'val': tensor(2.5617)}\n",
            "{'train': tensor(2.5353), 'val': tensor(2.5563)}\n",
            "{'train': tensor(2.5216), 'val': tensor(2.5522)}\n",
            "{'train': tensor(2.5297), 'val': tensor(2.5620)}\n",
            "{'train': tensor(2.5217), 'val': tensor(2.5607)}\n",
            "{'train': tensor(2.5374), 'val': tensor(2.5600)}\n",
            "{'train': tensor(2.5193), 'val': tensor(2.5692)}\n",
            "{'train': tensor(2.5231), 'val': tensor(2.5608)}\n",
            "{'train': tensor(2.5294), 'val': tensor(2.5635)}\n",
            "{'train': tensor(2.5268), 'val': tensor(2.5713)}\n",
            "{'train': tensor(2.5395), 'val': tensor(2.5624)}\n",
            "{'train': tensor(2.5367), 'val': tensor(2.5560)}\n",
            "{'train': tensor(2.5217), 'val': tensor(2.5643)}\n",
            "{'train': tensor(2.5312), 'val': tensor(2.5711)}\n",
            "{'train': tensor(2.5126), 'val': tensor(2.5655)}\n",
            "{'train': tensor(2.5440), 'val': tensor(2.5824)}\n",
            "{'train': tensor(2.5172), 'val': tensor(2.5649)}\n",
            "{'train': tensor(2.5283), 'val': tensor(2.5633)}\n",
            "{'train': tensor(2.5208), 'val': tensor(2.5644)}\n",
            "{'train': tensor(2.5329), 'val': tensor(2.5498)}\n",
            "{'train': tensor(2.5255), 'val': tensor(2.5583)}\n",
            "{'train': tensor(2.5363), 'val': tensor(2.5510)}\n",
            "{'train': tensor(2.5161), 'val': tensor(2.5572)}\n",
            "{'train': tensor(2.5234), 'val': tensor(2.5473)}\n",
            "{'train': tensor(2.5221), 'val': tensor(2.5508)}\n",
            "{'train': tensor(2.5323), 'val': tensor(2.5612)}\n",
            "{'train': tensor(2.5139), 'val': tensor(2.5647)}\n",
            "{'train': tensor(2.5283), 'val': tensor(2.5633)}\n",
            "{'train': tensor(2.5260), 'val': tensor(2.5668)}\n",
            "{'train': tensor(2.5131), 'val': tensor(2.5576)}\n",
            "{'train': tensor(2.5250), 'val': tensor(2.5537)}\n",
            "{'train': tensor(2.5246), 'val': tensor(2.5647)}\n",
            "{'train': tensor(2.5261), 'val': tensor(2.5664)}\n",
            "{'train': tensor(2.5259), 'val': tensor(2.5705)}\n",
            "{'train': tensor(2.5208), 'val': tensor(2.5599)}\n",
            "{'train': tensor(2.5128), 'val': tensor(2.5730)}\n",
            "{'train': tensor(2.5286), 'val': tensor(2.5445)}\n",
            "{'train': tensor(2.5250), 'val': tensor(2.5610)}\n",
            "{'train': tensor(2.5395), 'val': tensor(2.5460)}\n",
            "{'train': tensor(2.5206), 'val': tensor(2.5613)}\n",
            "{'train': tensor(2.5264), 'val': tensor(2.5501)}\n",
            "{'train': tensor(2.5107), 'val': tensor(2.5499)}\n",
            "{'train': tensor(2.5280), 'val': tensor(2.5706)}\n",
            "{'train': tensor(2.5305), 'val': tensor(2.5526)}\n",
            "{'train': tensor(2.5357), 'val': tensor(2.5532)}\n",
            "{'train': tensor(2.5155), 'val': tensor(2.5563)}\n",
            "{'train': tensor(2.5258), 'val': tensor(2.5531)}\n",
            "{'train': tensor(2.5142), 'val': tensor(2.5523)}\n",
            "{'train': tensor(2.5193), 'val': tensor(2.5565)}\n",
            "{'train': tensor(2.5294), 'val': tensor(2.5531)}\n",
            "{'train': tensor(2.5158), 'val': tensor(2.5492)}\n",
            "{'train': tensor(2.5264), 'val': tensor(2.5557)}\n",
            "{'train': tensor(2.5402), 'val': tensor(2.5704)}\n",
            "{'train': tensor(2.5347), 'val': tensor(2.5530)}\n",
            "{'train': tensor(2.5334), 'val': tensor(2.5679)}\n",
            "{'train': tensor(2.5306), 'val': tensor(2.5544)}\n",
            "{'train': tensor(2.5319), 'val': tensor(2.5414)}\n",
            "{'train': tensor(2.5343), 'val': tensor(2.5545)}\n",
            "{'train': tensor(2.5185), 'val': tensor(2.5468)}\n",
            "{'train': tensor(2.5319), 'val': tensor(2.5520)}\n",
            "{'train': tensor(2.5132), 'val': tensor(2.5598)}\n",
            "{'train': tensor(2.5371), 'val': tensor(2.5443)}\n",
            "{'train': tensor(2.5318), 'val': tensor(2.5520)}\n",
            "{'train': tensor(2.5225), 'val': tensor(2.5456)}\n",
            "{'train': tensor(2.5284), 'val': tensor(2.5427)}\n",
            "{'train': tensor(2.5323), 'val': tensor(2.5439)}\n",
            "{'train': tensor(2.5258), 'val': tensor(2.5501)}\n",
            "{'train': tensor(2.5355), 'val': tensor(2.5550)}\n",
            "{'train': tensor(2.5302), 'val': tensor(2.5619)}\n",
            "{'train': tensor(2.5116), 'val': tensor(2.5610)}\n",
            "{'train': tensor(2.5280), 'val': tensor(2.5616)}\n",
            "{'train': tensor(2.5405), 'val': tensor(2.5666)}\n",
            "{'train': tensor(2.5176), 'val': tensor(2.5430)}\n",
            "{'train': tensor(2.5130), 'val': tensor(2.5617)}\n",
            "{'train': tensor(2.5283), 'val': tensor(2.5530)}\n",
            "{'train': tensor(2.5268), 'val': tensor(2.5477)}\n",
            "{'train': tensor(2.5112), 'val': tensor(2.5411)}\n",
            "{'train': tensor(2.5227), 'val': tensor(2.5679)}\n",
            "{'train': tensor(2.5170), 'val': tensor(2.5571)}\n",
            "{'train': tensor(2.5224), 'val': tensor(2.5478)}\n",
            "{'train': tensor(2.5322), 'val': tensor(2.5421)}\n",
            "{'train': tensor(2.5289), 'val': tensor(2.5628)}\n",
            "{'train': tensor(2.5156), 'val': tensor(2.5578)}\n",
            "{'train': tensor(2.5176), 'val': tensor(2.5577)}\n",
            "{'train': tensor(2.5381), 'val': tensor(2.5631)}\n",
            "{'train': tensor(2.5293), 'val': tensor(2.5468)}\n",
            "{'train': tensor(2.5240), 'val': tensor(2.5566)}\n",
            "{'train': tensor(2.5139), 'val': tensor(2.5634)}\n",
            "{'train': tensor(2.5062), 'val': tensor(2.5524)}\n",
            "{'train': tensor(2.5298), 'val': tensor(2.5623)}\n",
            "{'train': tensor(2.5149), 'val': tensor(2.5630)}\n",
            "{'train': tensor(2.5158), 'val': tensor(2.5572)}\n",
            "{'train': tensor(2.5162), 'val': tensor(2.5552)}\n",
            "{'train': tensor(2.5233), 'val': tensor(2.5646)}\n",
            "{'train': tensor(2.5255), 'val': tensor(2.5625)}\n",
            "{'train': tensor(2.5272), 'val': tensor(2.5587)}\n",
            "{'train': tensor(2.5316), 'val': tensor(2.5734)}\n",
            "{'train': tensor(2.5249), 'val': tensor(2.5582)}\n",
            "{'train': tensor(2.5282), 'val': tensor(2.5653)}\n",
            "{'train': tensor(2.5303), 'val': tensor(2.5467)}\n",
            "{'train': tensor(2.5181), 'val': tensor(2.5587)}\n",
            "{'train': tensor(2.5180), 'val': tensor(2.5618)}\n",
            "{'train': tensor(2.5350), 'val': tensor(2.5664)}\n",
            "{'train': tensor(2.5159), 'val': tensor(2.5629)}\n",
            "{'train': tensor(2.5223), 'val': tensor(2.5470)}\n",
            "{'train': tensor(2.5189), 'val': tensor(2.5684)}\n",
            "{'train': tensor(2.5137), 'val': tensor(2.5534)}\n",
            "{'train': tensor(2.5295), 'val': tensor(2.5741)}\n",
            "{'train': tensor(2.5134), 'val': tensor(2.5425)}\n",
            "{'train': tensor(2.5225), 'val': tensor(2.5589)}\n",
            "{'train': tensor(2.5126), 'val': tensor(2.5372)}\n",
            "{'train': tensor(2.5220), 'val': tensor(2.5576)}\n",
            "{'train': tensor(2.5344), 'val': tensor(2.5557)}\n",
            "{'train': tensor(2.5308), 'val': tensor(2.5630)}\n",
            "{'train': tensor(2.5267), 'val': tensor(2.5600)}\n",
            "{'train': tensor(2.5055), 'val': tensor(2.5692)}\n",
            "{'train': tensor(2.5132), 'val': tensor(2.5393)}\n",
            "{'train': tensor(2.5176), 'val': tensor(2.5651)}\n",
            "{'train': tensor(2.5328), 'val': tensor(2.5450)}\n",
            "{'train': tensor(2.5224), 'val': tensor(2.5467)}\n",
            "{'train': tensor(2.5178), 'val': tensor(2.5578)}\n",
            "{'train': tensor(2.5282), 'val': tensor(2.5597)}\n",
            "{'train': tensor(2.5240), 'val': tensor(2.5554)}\n",
            "{'train': tensor(2.5290), 'val': tensor(2.5531)}\n",
            "{'train': tensor(2.5367), 'val': tensor(2.5484)}\n",
            "{'train': tensor(2.5419), 'val': tensor(2.5396)}\n",
            "{'train': tensor(2.5179), 'val': tensor(2.5527)}\n",
            "{'train': tensor(2.5228), 'val': tensor(2.5607)}\n",
            "{'train': tensor(2.5316), 'val': tensor(2.5514)}\n",
            "{'train': tensor(2.5166), 'val': tensor(2.5609)}\n",
            "{'train': tensor(2.5169), 'val': tensor(2.5380)}\n",
            "{'train': tensor(2.5311), 'val': tensor(2.5449)}\n",
            "{'train': tensor(2.5215), 'val': tensor(2.5496)}\n",
            "{'train': tensor(2.5209), 'val': tensor(2.5494)}\n",
            "{'train': tensor(2.5199), 'val': tensor(2.5621)}\n",
            "{'train': tensor(2.5166), 'val': tensor(2.5439)}\n",
            "{'train': tensor(2.5186), 'val': tensor(2.5491)}\n",
            "{'train': tensor(2.5214), 'val': tensor(2.5544)}\n",
            "{'train': tensor(2.5239), 'val': tensor(2.5457)}\n",
            "{'train': tensor(2.5122), 'val': tensor(2.5477)}\n",
            "{'train': tensor(2.5199), 'val': tensor(2.5762)}\n",
            "{'train': tensor(2.5352), 'val': tensor(2.5459)}\n",
            "{'train': tensor(2.5212), 'val': tensor(2.5342)}\n",
            "{'train': tensor(2.5262), 'val': tensor(2.5430)}\n",
            "{'train': tensor(2.5228), 'val': tensor(2.5591)}\n",
            "{'train': tensor(2.5160), 'val': tensor(2.5445)}\n",
            "{'train': tensor(2.5276), 'val': tensor(2.5442)}\n",
            "{'train': tensor(2.5173), 'val': tensor(2.5405)}\n",
            "{'train': tensor(2.5284), 'val': tensor(2.5567)}\n",
            "{'train': tensor(2.5203), 'val': tensor(2.5481)}\n",
            "{'train': tensor(2.5352), 'val': tensor(2.5441)}\n",
            "{'train': tensor(2.5229), 'val': tensor(2.5477)}\n",
            "{'train': tensor(2.5323), 'val': tensor(2.5542)}\n",
            "{'train': tensor(2.5071), 'val': tensor(2.5478)}\n",
            "{'train': tensor(2.5125), 'val': tensor(2.5573)}\n",
            "{'train': tensor(2.5177), 'val': tensor(2.5699)}\n",
            "{'train': tensor(2.5176), 'val': tensor(2.5429)}\n",
            "{'train': tensor(2.5310), 'val': tensor(2.5433)}\n",
            "{'train': tensor(2.5153), 'val': tensor(2.5621)}\n",
            "{'train': tensor(2.5198), 'val': tensor(2.5516)}\n",
            "{'train': tensor(2.5079), 'val': tensor(2.5483)}\n",
            "{'train': tensor(2.5275), 'val': tensor(2.5614)}\n",
            "{'train': tensor(2.5247), 'val': tensor(2.5381)}\n",
            "{'train': tensor(2.5148), 'val': tensor(2.5527)}\n",
            "{'train': tensor(2.5176), 'val': tensor(2.5441)}\n",
            "{'train': tensor(2.5256), 'val': tensor(2.5415)}\n",
            "{'train': tensor(2.5254), 'val': tensor(2.5473)}\n",
            "{'train': tensor(2.5233), 'val': tensor(2.5579)}\n",
            "{'train': tensor(2.5262), 'val': tensor(2.5551)}\n",
            "{'train': tensor(2.5358), 'val': tensor(2.5597)}\n",
            "{'train': tensor(2.5159), 'val': tensor(2.5610)}\n",
            "{'train': tensor(2.5137), 'val': tensor(2.5404)}\n",
            "{'train': tensor(2.5184), 'val': tensor(2.5596)}\n",
            "{'train': tensor(2.5157), 'val': tensor(2.5557)}\n",
            "{'train': tensor(2.5216), 'val': tensor(2.5502)}\n",
            "{'train': tensor(2.5211), 'val': tensor(2.5522)}\n",
            "{'train': tensor(2.5374), 'val': tensor(2.5515)}\n",
            "{'train': tensor(2.5126), 'val': tensor(2.5616)}\n",
            "{'train': tensor(2.5170), 'val': tensor(2.5524)}\n",
            "{'train': tensor(2.5146), 'val': tensor(2.5512)}\n",
            "{'train': tensor(2.5130), 'val': tensor(2.5539)}\n",
            "{'train': tensor(2.5185), 'val': tensor(2.5429)}\n",
            "{'train': tensor(2.5324), 'val': tensor(2.5482)}\n",
            "{'train': tensor(2.5415), 'val': tensor(2.5399)}\n",
            "{'train': tensor(2.5133), 'val': tensor(2.5535)}\n",
            "{'train': tensor(2.5270), 'val': tensor(2.5483)}\n",
            "{'train': tensor(2.5184), 'val': tensor(2.5604)}\n",
            "{'train': tensor(2.5316), 'val': tensor(2.5503)}\n",
            "{'train': tensor(2.5384), 'val': tensor(2.5428)}\n",
            "{'train': tensor(2.4972), 'val': tensor(2.5623)}\n",
            "{'train': tensor(2.5165), 'val': tensor(2.5589)}\n",
            "{'train': tensor(2.5274), 'val': tensor(2.5466)}\n",
            "{'train': tensor(2.5201), 'val': tensor(2.5462)}\n",
            "{'train': tensor(2.5138), 'val': tensor(2.5476)}\n",
            "{'train': tensor(2.5189), 'val': tensor(2.5537)}\n",
            "{'train': tensor(2.5293), 'val': tensor(2.5527)}\n",
            "{'train': tensor(2.5143), 'val': tensor(2.5517)}\n",
            "{'train': tensor(2.5116), 'val': tensor(2.5509)}\n",
            "{'train': tensor(2.5452), 'val': tensor(2.5659)}\n",
            "{'train': tensor(2.5202), 'val': tensor(2.5508)}\n",
            "{'train': tensor(2.5285), 'val': tensor(2.5545)}\n",
            "{'train': tensor(2.5285), 'val': tensor(2.5585)}\n",
            "{'train': tensor(2.5238), 'val': tensor(2.5487)}\n",
            "{'train': tensor(2.5145), 'val': tensor(2.5459)}\n",
            "{'train': tensor(2.5293), 'val': tensor(2.5491)}\n",
            "{'train': tensor(2.5088), 'val': tensor(2.5669)}\n",
            "{'train': tensor(2.5157), 'val': tensor(2.5444)}\n",
            "{'train': tensor(2.5150), 'val': tensor(2.5647)}\n",
            "{'train': tensor(2.5352), 'val': tensor(2.5472)}\n",
            "{'train': tensor(2.5337), 'val': tensor(2.5405)}\n",
            "{'train': tensor(2.5230), 'val': tensor(2.5603)}\n",
            "{'train': tensor(2.5112), 'val': tensor(2.5491)}\n",
            "{'train': tensor(2.5258), 'val': tensor(2.5534)}\n",
            "{'train': tensor(2.5219), 'val': tensor(2.5410)}\n",
            "{'train': tensor(2.5105), 'val': tensor(2.5587)}\n",
            "{'train': tensor(2.5134), 'val': tensor(2.5397)}\n",
            "{'train': tensor(2.5182), 'val': tensor(2.5401)}\n",
            "{'train': tensor(2.5154), 'val': tensor(2.5471)}\n",
            "{'train': tensor(2.5218), 'val': tensor(2.5546)}\n",
            "{'train': tensor(2.5220), 'val': tensor(2.5467)}\n",
            "{'train': tensor(2.5242), 'val': tensor(2.5593)}\n",
            "{'train': tensor(2.5280), 'val': tensor(2.5494)}\n",
            "{'train': tensor(2.5314), 'val': tensor(2.5518)}\n",
            "{'train': tensor(2.5149), 'val': tensor(2.5605)}\n",
            "{'train': tensor(2.5239), 'val': tensor(2.5654)}\n",
            "{'train': tensor(2.5169), 'val': tensor(2.5544)}\n",
            "{'train': tensor(2.5135), 'val': tensor(2.5497)}\n",
            "{'train': tensor(2.5221), 'val': tensor(2.5499)}\n",
            "{'train': tensor(2.5213), 'val': tensor(2.5564)}\n",
            "{'train': tensor(2.5141), 'val': tensor(2.5507)}\n",
            "{'train': tensor(2.5134), 'val': tensor(2.5464)}\n",
            "{'train': tensor(2.5184), 'val': tensor(2.5478)}\n",
            "{'train': tensor(2.5255), 'val': tensor(2.5472)}\n",
            "{'train': tensor(2.5052), 'val': tensor(2.5596)}\n",
            "{'train': tensor(2.5253), 'val': tensor(2.5699)}\n",
            "{'train': tensor(2.5296), 'val': tensor(2.5458)}\n",
            "{'train': tensor(2.4968), 'val': tensor(2.5573)}\n",
            "{'train': tensor(2.5303), 'val': tensor(2.5354)}\n",
            "{'train': tensor(2.5145), 'val': tensor(2.5509)}\n",
            "{'train': tensor(2.5108), 'val': tensor(2.5417)}\n",
            "{'train': tensor(2.5120), 'val': tensor(2.5390)}\n",
            "{'train': tensor(2.5209), 'val': tensor(2.5523)}\n",
            "{'train': tensor(2.5146), 'val': tensor(2.5546)}\n",
            "{'train': tensor(2.5193), 'val': tensor(2.5620)}\n",
            "{'train': tensor(2.5186), 'val': tensor(2.5528)}\n",
            "{'train': tensor(2.5182), 'val': tensor(2.5555)}\n",
            "{'train': tensor(2.5172), 'val': tensor(2.5645)}\n",
            "{'train': tensor(2.5344), 'val': tensor(2.5539)}\n",
            "{'train': tensor(2.5235), 'val': tensor(2.5585)}\n",
            "{'train': tensor(2.5193), 'val': tensor(2.5430)}\n",
            "{'train': tensor(2.5270), 'val': tensor(2.5465)}\n",
            "{'train': tensor(2.5263), 'val': tensor(2.5436)}\n",
            "{'train': tensor(2.5273), 'val': tensor(2.5477)}\n",
            "{'train': tensor(2.5184), 'val': tensor(2.5562)}\n",
            "{'train': tensor(2.5187), 'val': tensor(2.5491)}\n",
            "{'train': tensor(2.5108), 'val': tensor(2.5508)}\n",
            "{'train': tensor(2.5117), 'val': tensor(2.5549)}\n",
            "{'train': tensor(2.5269), 'val': tensor(2.5490)}\n",
            "{'train': tensor(2.5258), 'val': tensor(2.5502)}\n",
            "{'train': tensor(2.5294), 'val': tensor(2.5458)}\n",
            "{'train': tensor(2.5200), 'val': tensor(2.5481)}\n",
            "{'train': tensor(2.5108), 'val': tensor(2.5480)}\n",
            "{'train': tensor(2.5126), 'val': tensor(2.5535)}\n",
            "{'train': tensor(2.5198), 'val': tensor(2.5455)}\n",
            "{'train': tensor(2.5124), 'val': tensor(2.5564)}\n",
            "{'train': tensor(2.5173), 'val': tensor(2.5489)}\n",
            "{'train': tensor(2.5301), 'val': tensor(2.5679)}\n",
            "{'train': tensor(2.5293), 'val': tensor(2.5459)}\n",
            "{'train': tensor(2.5198), 'val': tensor(2.5424)}\n",
            "{'train': tensor(2.5261), 'val': tensor(2.5494)}\n",
            "{'train': tensor(2.5249), 'val': tensor(2.5459)}\n",
            "{'train': tensor(2.5228), 'val': tensor(2.5546)}\n",
            "{'train': tensor(2.5286), 'val': tensor(2.5422)}\n",
            "{'train': tensor(2.5210), 'val': tensor(2.5463)}\n",
            "{'train': tensor(2.5143), 'val': tensor(2.5636)}\n",
            "{'train': tensor(2.5311), 'val': tensor(2.5523)}\n",
            "{'train': tensor(2.5240), 'val': tensor(2.5369)}\n",
            "{'train': tensor(2.5228), 'val': tensor(2.5458)}\n",
            "{'train': tensor(2.5170), 'val': tensor(2.5384)}\n",
            "{'train': tensor(2.5228), 'val': tensor(2.5617)}\n",
            "{'train': tensor(2.5239), 'val': tensor(2.5538)}\n",
            "{'train': tensor(2.5203), 'val': tensor(2.5619)}\n",
            "{'train': tensor(2.5169), 'val': tensor(2.5510)}\n",
            "{'train': tensor(2.5102), 'val': tensor(2.5389)}\n",
            "{'train': tensor(2.5225), 'val': tensor(2.5546)}\n",
            "{'train': tensor(2.5162), 'val': tensor(2.5433)}\n",
            "{'train': tensor(2.5097), 'val': tensor(2.5562)}\n",
            "{'train': tensor(2.5172), 'val': tensor(2.5699)}\n",
            "{'train': tensor(2.5266), 'val': tensor(2.5598)}\n",
            "{'train': tensor(2.5232), 'val': tensor(2.5524)}\n",
            "{'train': tensor(2.5227), 'val': tensor(2.5475)}\n",
            "{'train': tensor(2.5164), 'val': tensor(2.5368)}\n",
            "{'train': tensor(2.5080), 'val': tensor(2.5560)}\n",
            "{'train': tensor(2.4992), 'val': tensor(2.5464)}\n",
            "{'train': tensor(2.5095), 'val': tensor(2.5459)}\n",
            "{'train': tensor(2.5205), 'val': tensor(2.5438)}\n",
            "{'train': tensor(2.5107), 'val': tensor(2.5396)}\n",
            "{'train': tensor(2.5156), 'val': tensor(2.5495)}\n",
            "{'train': tensor(2.5174), 'val': tensor(2.5520)}\n",
            "{'train': tensor(2.5096), 'val': tensor(2.5510)}\n",
            "{'train': tensor(2.5275), 'val': tensor(2.5551)}\n",
            "{'train': tensor(2.5137), 'val': tensor(2.5351)}\n",
            "{'train': tensor(2.5070), 'val': tensor(2.5601)}\n",
            "{'train': tensor(2.5263), 'val': tensor(2.5449)}\n",
            "{'train': tensor(2.5340), 'val': tensor(2.5610)}\n",
            "{'train': tensor(2.5028), 'val': tensor(2.5465)}\n",
            "{'train': tensor(2.5056), 'val': tensor(2.5622)}\n",
            "{'train': tensor(2.5196), 'val': tensor(2.5461)}\n",
            "{'train': tensor(2.5037), 'val': tensor(2.5497)}\n",
            "{'train': tensor(2.5152), 'val': tensor(2.5479)}\n",
            "{'train': tensor(2.5073), 'val': tensor(2.5488)}\n",
            "{'train': tensor(2.5197), 'val': tensor(2.5394)}\n",
            "{'train': tensor(2.5101), 'val': tensor(2.5471)}\n",
            "{'train': tensor(2.5065), 'val': tensor(2.5514)}\n",
            "{'train': tensor(2.5055), 'val': tensor(2.5565)}\n",
            "{'train': tensor(2.5330), 'val': tensor(2.5406)}\n"
          ]
        }
      ],
      "source": [
        "#Training the model\n",
        "\n",
        "batch_size =32\n",
        "\n",
        "for iterations in range (1000):\n",
        "\n",
        "  #sample a batch of data\n",
        "  xb,yb = get_batch(\"train\") # We get a 32 batches of 8 character indicies. shape (32,8)\n",
        "\n",
        "  #Generate the predicted character and caluclate the loss by comparing to the actual target character yb\n",
        "  logits, loss = m(xb,yb) #This call the forward function.\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  loss = average_loss(100)\n",
        "\n",
        "  print(loss)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuXoamQ-H7Za",
        "outputId": "c55336cb-8f1e-4774-f11f-bf8db7582867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "E\n",
            "SGNT\n",
            "SSWTTFMWWTHY\n",
            "Y\n",
            "JAWB\n",
            "SYSTBI\n",
            "AA\n",
            "HWT\n",
            "TH\n",
            "I\n",
            "aEGTCW\n",
            "BT\n",
            "\n",
            "hB\n",
            "S\n",
            "JQATS\n",
            "WWPHIS\n",
            "SSBSTN\n",
            "HHCL\n",
            "T\n",
            "EWSISESWPA\n"
          ]
        }
      ],
      "source": [
        "#Generating a sequence with a slightly optimised model\n",
        "inputs = torch.zeros((1,1),dtype = torch.long)\n",
        "print(decode(m.generate(inputs,100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-CBHJodGO5F"
      },
      "source": [
        "<h2> The mathematical trick in self -attention </h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGXWTjTdv_9k",
        "outputId": "e6a8367f-3e3b-4c99-c14b-b4c4069c89d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The inputs in the 1st batch \n",
            " tensor([[-0.3376, -1.5154],\n",
            "        [-0.8523, -0.3037],\n",
            "        [-0.7137, -0.8800],\n",
            "        [ 0.1575, -0.6906],\n",
            "        [-0.7282,  1.1125],\n",
            "        [-1.0222,  0.2645],\n",
            "        [-0.2275,  0.3516],\n",
            "        [ 0.8941, -1.3865]])\n",
            "The averaged x_bag_of_words for the 1st batch \n",
            " tensor([[-0.3376, -1.5154],\n",
            "        [-0.5950, -0.9095],\n",
            "        [-0.6345, -0.8997],\n",
            "        [-0.4365, -0.8474],\n",
            "        [-0.4948, -0.4554],\n",
            "        [-0.5827, -0.3354],\n",
            "        [-0.5320, -0.2373],\n",
            "        [-0.3537, -0.3809]])\n"
          ]
        }
      ],
      "source": [
        "#Example of Naive Implementation of Averaging the past information (characters) to predict future character\n",
        "\n",
        "#Creating a dummy x dataset\n",
        "B,T,C = 4,8,2 #Batch, Time, Channels\n",
        "x = torch.randn((B,T,C))\n",
        "print(\"The inputs in the 1st batch \\n\",x[0,:,:])\n",
        "\n",
        "#Averaging over the time dimension in each channel in each batch.\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B): #Iterating over each batch\n",
        "  for t in range(T):\n",
        "    x_previous = x[b,:t+1] # All characters in batch b from the 0th time index (past) to the current time index\n",
        "    #print(x_previous.shape) x_previous is of shape (t,C) where t is current time index in the for loop.\n",
        "    xbow[b,t] = torch.mean(x_previous,0) # 0 axis represents the averaging over the time axis\n",
        "    #print(torch.mean(x_previous,1)) For the curious what does averaging over the channels (dim =1) look like !\n",
        "print(\"The averaged x_bag_of_words for the 1st batch \\n\", xbow[0,:,:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGsfQWqkM9iP",
        "outputId": "df850fa8-a91a-4649-d3c1-e178c71e3dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The weights are \n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "tensor([[-0.3376, -1.5154],\n",
            "        [-0.5950, -0.9095],\n",
            "        [-0.6345, -0.8997],\n",
            "        [-0.4365, -0.8474],\n",
            "        [-0.4948, -0.4554],\n",
            "        [-0.5827, -0.3354],\n",
            "        [-0.5320, -0.2373],\n",
            "        [-0.3537, -0.3809]])\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "#More efficient implementation of the averaging with Matrix Multiplication\n",
        "'''Matrix Multiplication with a lower triangle matrix [[1,0,0],\n",
        "                                                     [1,1,0],\n",
        "                                                     [1,1,1]] with a input vector results in the summation of the past elemnets in time dimension\n",
        "While normalising the lower triangular matrix along the rows such that each row sum up to 1 results in the averaging over the past elements in the time dimension\n",
        " weights = [[1,0,0],\n",
        "            [0.5,0.5,0],\n",
        "            [0.33,0.33,0.33]]'''\n",
        "\n",
        "weights = torch.tril(torch.ones(T,T)) # A lower trianglular matrix\n",
        "weights = weights/weights.sum(dim = 1,keepdim=True) #Summing along the rows and keep dims = True to preserve the dimension\n",
        "print(\"The weights are \\n\",weights)\n",
        "\n",
        "#Matrix Multiplication to calculate the average\n",
        "xbow2 = weights @ x         #(T,T) @ (B,T,C) ==> (B,T,T) @ (B,T,C) pytorch adds a Batch dimension to weights by copying the same TxT weights acroos the batches.\n",
        "print(xbow2[0])             #xbow2 is of shape (B,T,C)\n",
        "print(torch.allclose(xbow,xbow2)) #True Tells us that xbow and xbow2 obtained through naive and mat multiplication give same results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1NPh1ONYW3O"
      },
      "source": [
        "<h3>Normalising using the softmax function </h3>\n",
        "\n",
        "Softmax function = $\\frac{e^x}{\\sum e^x}$\n",
        "\n",
        "On applying the softmax function over the rows (dim =1):\n",
        "\n",
        "In the first row, the one zero $e^0 = 1$ and the other -inf results in 0. Thus it results in $\\frac{1}{1+0+..+0} = 1$\n",
        "\n",
        "In the second row, there are two zeros and it results in $\\frac{1}{1+1+0..0} = \\frac{1}{2}$\n",
        "\n",
        "And similarly in third row $\\frac{1}{3}$ and so on till the eight row.\n",
        "\n",
        "<h3> Why we use Softmax function for normalising rather than the previous version? </h3>\n",
        "\n",
        "We may feel the previous version of creating a lower triangular matrix with ones and normalising them was a more intutive approach.\n",
        "\n",
        "But when we train the model we may consider the weights as an affinity between the characetrs where some characters might find the another character next to it more interesting. This affinity is calculated based on the past characters only and this is why mask the future chracter indices with '-inf' at each time step. Thus these affinity modelled as weights are intialized to zero at the begining and are then trained to be updated based on the data fed to the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlAJt0UKTbHB",
        "outputId": "ed00adc6-dc6e-478b-f271-3a4bb613d67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The initial weight tensor  tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "The normalised weights  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "The aggregated first batch of bag of words tensor([[-0.3376, -1.5154],\n",
            "        [-0.5950, -0.9095],\n",
            "        [-0.6345, -0.8997],\n",
            "        [-0.4365, -0.8474],\n",
            "        [-0.4948, -0.4554],\n",
            "        [-0.5827, -0.3354],\n",
            "        [-0.5320, -0.2373],\n",
            "        [-0.3537, -0.3809]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Implementation with Softmax function\n",
        "\n",
        "lwr_triangle_mat = torch.tril(torch.ones(T,T)) #Creating a lower triangular matrix\n",
        "weights = torch.zeros(T,T) # Intiating the weights to zeros\n",
        "weights = weights.masked_fill(lwr_triangle_mat == 0, float('-inf'))\n",
        "print(\"The initial weight tensor \",weights) # This for a lower traingle matrix where 1s are replaced by 0 and the 0s are replaced by -inf\n",
        "weights = F.softmax(weights, dim = 1) # Normalising using the softmax function\n",
        "print(\"The normalised weights \",weights)\n",
        "#Aggregating over thee inputs with matrix multiplication\n",
        "xbow3 = weights @ x\n",
        "print(\"The aggregated first batch of bag of words\",xbow[0])\n",
        "torch.allclose(xbow,xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbnjoo8cYeGc",
        "outputId": "b857241f-653a-4430-d9c3-b3f302f853c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007,\n",
            "          -0.5239, -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,\n",
            "           0.2862,  0.5710],\n",
            "         [ 0.2507,  0.1815, -0.0388, -0.2458, -0.1356,  0.2369, -0.1588,\n",
            "          -0.3209, -0.4772,  0.4530,  0.4388, -0.3604, -0.0859, -0.0803,\n",
            "           0.1115,  0.9138],\n",
            "         [ 0.3288,  0.0950, -0.1875, -0.0916, -0.0079,  0.0883, -0.0678,\n",
            "          -0.1830, -0.4008,  0.0761,  0.3542, -0.1453, -0.1970, -0.0976,\n",
            "           0.0109,  1.0278],\n",
            "         [ 0.6067, -0.4271, -0.2246,  0.2273, -0.1100, -0.2183, -0.3709,\n",
            "          -0.1340, -0.1130,  0.6494,  0.6441, -0.1387,  0.2489,  0.2713,\n",
            "          -0.0351,  1.2031],\n",
            "         [ 0.2010,  0.8507,  0.6533,  0.2228,  0.3173,  0.8365,  0.6526,\n",
            "           0.3822, -0.6315, -1.2205, -0.4374, -0.2859, -0.9985,  0.1108,\n",
            "          -0.1001,  0.5346],\n",
            "         [ 0.1453,  0.4755,  0.1447, -0.2496, -0.0209,  0.4674,  0.0808,\n",
            "          -0.2074, -0.5866,  0.0157,  0.1711, -0.3741, -0.3699, -0.1248,\n",
            "           0.1164,  0.7404],\n",
            "         [-0.2268,  0.2806, -0.0834,  0.2215,  0.1804,  0.2529, -0.0778,\n",
            "          -0.2663, -0.1468,  0.1037,  0.0856,  0.1898, -0.0721, -0.0397,\n",
            "           0.3974,  0.4161],\n",
            "         [-0.1450,  0.2375, -0.1882,  0.3479,  0.1843,  0.1369, -0.0581,\n",
            "           0.1339, -0.0594, -0.0362,  0.0767,  0.2613,  0.0609, -0.1358,\n",
            "           0.0764,  0.3417]],\n",
            "\n",
            "        [[-1.3254,  1.1236,  0.2293, -0.2997, -0.0076,  0.7936,  0.8958,\n",
            "           0.3965, -0.6661, -0.2184, -1.3539,  0.4124,  0.9601, -1.0805,\n",
            "          -0.3975, -0.4444],\n",
            "         [-0.6782,  0.2166,  0.1325,  0.0337, -0.0622,  0.3393,  0.4923,\n",
            "          -0.0432, -0.3417, -0.0805, -0.0910,  0.5015,  0.2865, -0.6299,\n",
            "          -0.2856, -0.4583],\n",
            "         [-1.0932,  0.5732,  0.3181, -0.1236,  0.0993,  0.4099,  0.5209,\n",
            "           0.2445, -0.1716, -0.0925, -0.4941,  0.4624,  0.7222, -0.5886,\n",
            "          -0.0722, -0.2854],\n",
            "         [-0.2692, -0.3212,  0.1692,  0.2796, -0.0396,  0.1096,  0.2002,\n",
            "          -0.2547, -0.0769,  0.0629,  0.6160,  0.5256, -0.0585, -0.2775,\n",
            "          -0.1455, -0.3959],\n",
            "         [-0.9002,  0.7484,  0.3542, -0.1426, -0.0619,  0.5054,  0.5385,\n",
            "           0.4772, -0.1972, -0.0022, -0.4076,  0.3338,  0.7242, -0.5826,\n",
            "          -0.0953, -0.3755],\n",
            "         [-0.9485,  0.5534,  0.3723, -0.1054,  0.0385,  0.3561,  0.4288,\n",
            "           0.3590, -0.0392, -0.0078, -0.2295,  0.3925,  0.6895, -0.4531,\n",
            "           0.0200, -0.2877],\n",
            "         [-0.1033,  0.1116,  0.2029,  0.0800, -0.3062,  0.1282,  0.1350,\n",
            "           0.1713, -0.0601,  0.2307,  0.6556,  0.1717,  0.1498, -0.3954,\n",
            "          -0.1136, -0.5638],\n",
            "         [ 0.3211,  0.2505,  0.4162, -0.1897, -0.9094, -0.4732, -0.5461,\n",
            "          -0.1402, -0.1977,  0.6464,  0.3451, -1.0749,  0.3546, -0.9315,\n",
            "           0.3150, -0.8036]],\n",
            "\n",
            "        [[ 0.0689,  1.2248, -0.4119, -0.1705, -0.6922, -0.2920,  1.2704,\n",
            "          -0.6860,  0.4380, -0.2637,  0.1153,  1.1676, -0.7214, -1.2308,\n",
            "           0.8382, -0.5599],\n",
            "         [-0.3586,  0.7538, -0.2084, -0.1405, -0.6134, -0.4620,  0.4376,\n",
            "          -0.3932,  0.4539, -0.2012,  0.1770,  0.5319, -0.4305, -0.8630,\n",
            "           0.4466, -0.0545],\n",
            "         [-0.3677,  0.6115, -0.1984, -0.1357, -0.5644, -0.4867,  0.3944,\n",
            "          -0.4084,  0.4726, -0.1723,  0.2934,  0.5107, -0.4501, -0.7446,\n",
            "           0.5277,  0.0154],\n",
            "         [-0.3990,  0.0805, -0.0982, -0.1297, -0.2354, -0.5444, -0.0031,\n",
            "          -0.3215,  0.4504,  0.0281,  0.5181,  0.2817, -0.4165, -0.2112,\n",
            "           0.6204,  0.2792],\n",
            "         [ 0.1649,  0.7081, -0.1468, -0.2100,  0.1909, -0.1943,  0.2485,\n",
            "          -0.1734,  0.0855,  0.2797, -0.3395,  0.5875, -0.4114, -0.3832,\n",
            "           0.3573, -0.3944],\n",
            "         [-0.1367,  0.6251, -0.3663, -0.0236, -0.2402, -0.2556,  0.6569,\n",
            "          -0.2531,  0.3443, -0.0638,  0.0646,  0.8383, -0.4728, -0.6836,\n",
            "           0.5997, -0.4341],\n",
            "         [-0.5889,  0.3277, -0.4244,  0.0367, -0.5843, -0.5685,  0.8307,\n",
            "          -0.1640,  0.2316, -0.1908,  0.4925,  0.9825, -0.5326, -0.9373,\n",
            "           0.4137, -0.3095],\n",
            "         [ 0.0878,  0.1162,  0.1891,  0.2540, -0.3062, -0.3498, -0.3664,\n",
            "          -0.0187, -0.0367,  0.3548,  0.3735,  0.1812, -0.0254,  0.2142,\n",
            "           0.0075,  0.3483]],\n",
            "\n",
            "        [[ 0.0972,  0.0573, -0.1047, -0.0467, -0.1401, -0.8413, -0.1362,\n",
            "          -0.6747, -0.2154,  1.0993,  0.2343,  0.0326, -0.1852,  0.1478,\n",
            "          -0.6104,  1.5391],\n",
            "         [ 0.1545, -0.1026, -0.2492,  0.1041, -0.1627, -0.7844, -0.4398,\n",
            "          -0.6849, -0.2095,  0.8632,  0.2355,  0.0137, -0.1762,  0.1264,\n",
            "          -0.6703,  1.3623],\n",
            "         [ 0.3591, -0.6735, -0.7650,  0.6430, -0.2442, -0.5798, -1.5236,\n",
            "          -0.7217, -0.1891,  0.0172,  0.2393, -0.0537, -0.1441,  0.0507,\n",
            "          -0.8836,  0.7290],\n",
            "         [ 0.1893, -0.1428, -0.2553,  0.1990, -0.2454, -0.5450, -0.5237,\n",
            "          -0.6960, -0.2865,  0.3660,  0.1492,  0.0170, -0.1627,  0.1921,\n",
            "          -0.6224,  1.0241],\n",
            "         [ 0.0154,  0.5391,  0.3967, -0.5460, -0.2802, -0.0317,  0.7286,\n",
            "          -0.4372, -0.5495, -0.0808, -0.0840,  0.1312, -0.0962,  0.3494,\n",
            "          -0.2570,  0.6339],\n",
            "         [-0.0863, -0.0645,  0.1225, -0.2552, -0.0476, -0.2964, -0.2058,\n",
            "          -0.2407, -0.1796,  0.1652,  0.1846,  0.0306, -0.0122,  0.0399,\n",
            "          -0.4678,  0.4718],\n",
            "         [-0.3132,  0.5835,  0.5740, -0.8270, -0.0751, -0.1630,  0.3293,\n",
            "           0.2245, -0.0322, -0.0074, -0.0260, -0.6335,  0.3700,  0.3922,\n",
            "          -0.5330,  0.4694],\n",
            "         [-0.9743, -0.0109,  0.3368, -0.0521,  0.2160,  0.0250, -0.3730,\n",
            "           0.3319, -0.5372, -0.2131,  0.2343, -0.2207,  0.1196, -0.2035,\n",
            "          -0.5169,  0.0992]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Single Self Attention Block\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # Batch, Time, Channel dimension Channel is each character is encoded into a 32 length embedding\n",
        "x = torch.randn((B,T,C)) # Random input\n",
        "\n",
        "head_size =16  # HyperParameter\n",
        "#For each input we create 3 vectors - Query, Key and a Value\n",
        "Query = nn.Linear(C,head_size,bias=False) # A linear layer that creates a length 16 query vector for each input character\n",
        "Key = nn.Linear(C,head_size,bias=False)   #A linear layer that creates a length 16 key vector for each input character\n",
        "Value = nn.Linear(C, head_size,bias=False) #A linear layer that creates a length 16 value vector for each input character\n",
        "\n",
        "q = Query(x) #q => (B,T,16)\n",
        "k = Key(x)   #k => (B,T,16)\n",
        "v = Value(x) #V => (B,T,16)\n",
        "\n",
        "#Now the weights is what each character is looking for another character that it has most affinity to.\n",
        "#It is a dot product of queries and keys of each cahracter.\n",
        "weights = q @ k.transpose(2,1) * head_size ** (-0.5) # We can multiply the B batches of matrices of size (T,16) @ (T,16) so we transpose the key tensor.\n",
        "#We only have to transpose the last two dimensions and not the Batch dimension\n",
        "#print(weights) #(B,T,16) @ (B,16,T) => (B,T,T) Here T is 8.\n",
        "#These weights represent the affinity that a character at a postion might have to another character. If the wieghts are higher indicates higher affinity.\n",
        "# The multiplication with the 1/sqrt(head_size) results in diffused softmax rather than a sharp softmax similar to a one hot vector.\n",
        "\n",
        "#Now we normalise these weights using masking and the softmax function.\n",
        "lower_triangular_mask = torch.tril(torch.ones(T,T))\n",
        "weights = weights.masked_fill(lower_triangular_mask ==0 , float('-inf'))\n",
        "weights = F.softmax(weights,dim = 2)\n",
        "#print(weights) #Normalised weights\n",
        "\n",
        "out = weights @ v # We multiply it with the values of the x rather than the raw x (inputs) => (T,T) @(B,T,C) => (8,8) @ (4,8,16) =>(4,8,16)\n",
        "print(out) # shape of out(4,8,16)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa1v5xxoF9IY",
        "outputId": "fdb3cbb2-1e4c-44a3-bbd2-d2a9c4e72b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.841281 M Parameters\n",
            "step0: train loss 4.2345, val loss 4.2418\n",
            "step100: train loss 2.5153, val loss 2.5178\n",
            "step200: train loss 2.4748, val loss 2.4900\n",
            "step300: train loss 2.4498, val loss 2.4657\n",
            "step400: train loss 2.3777, val loss 2.4108\n",
            "step500: train loss 2.2869, val loss 2.3186\n",
            "step600: train loss 2.1836, val loss 2.2222\n",
            "step700: train loss 2.1062, val loss 2.1521\n",
            "step800: train loss 2.0351, val loss 2.1114\n",
            "step900: train loss 1.9685, val loss 2.0613\n",
            "step1000: train loss 1.9105, val loss 2.0151\n",
            "step1100: train loss 1.8559, val loss 1.9828\n",
            "step1200: train loss 1.8129, val loss 1.9477\n",
            "step1300: train loss 1.7803, val loss 1.9193\n",
            "step1400: train loss 1.7417, val loss 1.8993\n",
            "step1499: train loss 1.7134, val loss 1.8632\n",
            "\n",
            "Wouded Her-butind my a hesepcrounione\n",
            "My to ter dote me.\n",
            "\n",
            "My ROGHOMNIANDIANIUS:\n",
            "Hard my lordo your dive ome, le weed stain.\n",
            "God friasts blowicess it.\n",
            "I as pontly of dow en if is brer's ful haven are:\n",
            "Tahe houst that comet; you annot inquisong of clower.\n",
            "\n",
            "DUCKE OF You whe Was it.\n",
            "What,\n",
            "i' makeeppent Lether, lorry that town? Mant alaugne sentay\n",
            "Broth, have you her land somblenthen arm, mayquea;\n",
            "Her a fady arwilnisomed: hell, he' glay nifouch,\n",
            "waich a st Is in hould upor wind thicence, me\n",
            "For ber y\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 256\n",
        "n_embed = 128\n",
        "n_layers = 4\n",
        "n_head = 4\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iter = 1500\n",
        "eval_iters = 100\n",
        "dropout =0.2\n",
        "eval_interval = 500\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def average_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "#Single Head attention\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self,head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "    self.head_size = head_size\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape    #Batch, Time, Channel (batch_size,block_size,n_embed)\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    # All the above q,k,v are of shape (B,T,C)\n",
        "\n",
        "    #Calculating the weights that are the affinities which is represented by the product between query and key.\n",
        "    weights = q @ k.transpose(-2,-1)    #(B,T,C) @ (B,C,T)  === (B,T,T)\n",
        "    #Normalizing and scaling weights using the softmax function\n",
        "    weights = weights.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    weights = F.softmax(weights,dim = -1)     # Normalisation along the last dimension\n",
        "\n",
        "    weights = weights * self.head_size ** (-0.5)   # Scaling the weights to avoid sharpening at one character/token\n",
        "    weights = self.dropout(weights)\n",
        "    output = weights @ v\n",
        "    #print(output.shape)\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  #Multiple heads in parallel\n",
        "  def __init__(self,num_heads,head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # Create a list of heads as many as the num_heads\n",
        "    self.proj = nn.Linear(head_size * num_heads,n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "   #print(\"self.proj(out) \",out)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "                nn.Linear(n_embed, 4*n_embed),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(4*n_embed,n_embed),\n",
        "                nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "  #A transformer Block : Communication followed by Computation\n",
        "\n",
        "  def __init__(self,n_heads,n_embed):\n",
        "    super().__init__()\n",
        "    self.LayerNorm1 = nn.LayerNorm(n_embed)\n",
        "    head_size = n_embed//n_heads\n",
        "    self.self_attention = MultiHeadAttention(n_heads,head_size)\n",
        "    self.LayerNorm2 = nn.LayerNorm(n_embed)\n",
        "    self.feedforward = FeedForward(n_embed)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.self_attention(self.LayerNorm1(x)) #Adding x directly creates the residual connections\n",
        "    x = x + self.feedforward(self.LayerNorm2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "#GPT Language Model\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size,n_embed)\n",
        "\n",
        "    #self.selfatttention_head = Head(n_embed)          #Intializing the self attention head with head_size = n_embeds in case of a single head\n",
        "    #\n",
        "\n",
        "    self.blocks = nn.Sequential(*[Block(n_head,n_embed) for _ in range(n_layers)])\n",
        "    #The asterisk * unpacks the list generated by the list comprehension into separate arguments.\n",
        "    #This is often used when passing multiple arguments to a function or a class constructor that takes variable-length arguments (e.g., *args).\n",
        "\n",
        "    self.LayerNorm3 = nn.LayerNorm(n_embed)\n",
        "    self.languagemodel_head = nn.Linear(n_embed,vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, indices, targets= None):\n",
        "    #print(indices.shape)\n",
        "    B,T = indices.shape\n",
        "\n",
        "    #Performing Forward Pass\n",
        "    token_embed = self.token_embedding_table(indices)\n",
        "    position_embed = self.position_embedding_table(torch.arange(T))\n",
        "    x = token_embed + position_embed\n",
        "    #x = self.selfatttention_head(x)              # For single head - Calling the self attention head on inputs. The returned x will have token of x that has high attention compared to others.\n",
        "\n",
        "    x = self.blocks(x)\n",
        "    x = self.LayerNorm3(x)\n",
        "    logits = self.languagemodel_head(x)\n",
        "\n",
        "    #Calculating Loss\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      #Reshaping according to the cross-entropy loss function\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy (logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, indices, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      #print(\"indices\",indices)\n",
        "      index_cropped = indices[:,-block_size:]\n",
        "      #print(\"index_cropped \",index_cropped)\n",
        "      logits,loss = self(index_cropped) # logits => (B,T,C)\n",
        "      logits = logits[:,-1,:]     #(B,C)\n",
        "      probability = F.softmax(logits,dim = -1) # Softmax along the channels\n",
        "      next_index = torch.multinomial(probability,1,replacement=True) #(B,1)\n",
        "      indices = torch.cat((indices,next_index),dim=1) #(B,T+1)\n",
        "    return indices\n",
        "\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "\n",
        "#Inspecting model parameters\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, \"M Parameters\")\n",
        "\n",
        "#Optimizer for the model\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr = learning_rate)\n",
        "\n",
        "for iter in range(max_iter):\n",
        "  if iter % eval_iters == 0 or iter == max_iter-1:\n",
        "    losses = average_loss()\n",
        "    print(f'step{iter}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n",
        "\n",
        "  #Get the batch data\n",
        "  xb,yb = get_batch('train')\n",
        "\n",
        "  #Evaluate Losses\n",
        "  logits,loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "\n",
        "  #Calculate the gradients\n",
        "  loss.backward()\n",
        "  #Update the weights\n",
        "  optimizer.step()\n",
        "\n",
        "#generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D219WpOh2kG9"
      },
      "source": [
        "<h3> About AutoGrad - Automatic differentiation </h3>\n",
        "\n",
        "\n",
        "<h4> torch.no_grad </h4>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}